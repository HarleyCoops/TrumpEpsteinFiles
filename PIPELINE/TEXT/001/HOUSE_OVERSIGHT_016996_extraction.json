{
  "file_name": "HOUSE_OVERSIGHT_016996.txt",
  "file_path": "001\\HOUSE_OVERSIGHT_016996.txt",
  "content": {
    "full_text": "﻿Quantitative Analysis of Culture Using Millions of Digitized Books\nJean-Baptiste Michel, 1,2,3,4 *† Yuan Kui Shen, 5 Aviva Presser Aiden, 6 Adrian Veres, 7 Matthew K. Gray, 8 The Google Books\nTeam, 8 Joseph P. Pickett, 9 Dale Hoiberg, 10 Dan Clancy, 8 Peter Norvig, 8 Jon Orwant, 8 Steven Pinker, 4 Martin A. Nowak, 1,11,12\nErez Lieberman Aiden 1,12,13,14,15,16 *†\n1 Program for Evolutionary Dynamics, Harvard University, Cambridge, MA 02138, USA. 2 Institute for Quantitative Social\nSciences, Harvard University, Cambridge, MA 02138, USA. 3 Department of Psychology, Harvard University, Cambridge, MA\n02138, USA. 4 Department of Systems Biology, Harvard Medical School, Boston, MA 02115, USA. 5 Computer Science and\nArtificial Intelligence Laboratory, MIT, Cambridge, MA 02139, USA. 6 Harvard Medical School, Boston, MA, 02115, USA.\n7 Harvard College, Cambridge, MA 02138, USA. 8 Google, Inc., Mountain View, CA, 94043, USA. 9 Houghton Mifflin Harcourt,\nBoston, MA 02116, USA. 10 Encyclopaedia Britannica, Inc., Chicago, IL 60654, USA. 11 Dept of Organismic and Evolutionary\nBiology, Harvard University, Cambridge, MA 02138, USA. 12 Dept of Mathematics, Harvard University, Cambridge, MA\n02138, USA. 13 Broad Institute of Harvard and MIT, Harvard University, Cambridge, MA 02138, USA. 14 School of Engineering\nand Applied Sciences, Harvard University, Cambridge, MA 02138, USA. 15 Harvard Society of Fellows, Harvard University,\nCambridge, MA 02138, USA. 16 Laboratory-at-Large, Harvard University, Cambridge, MA 02138, USA.\n*These authors contributed equally to this work.\n†To whom correspondence should be addressed. E-mail: jb.michel@gmail.com (J.B.M.); erez@erez.com (E.A.).\nWe constructed a corpus of digitized texts containing\nabout 4% of all books ever printed. Analysis of this\ncorpus enables us to investigate cultural trends\nquantitatively. We survey the vast terrain of\n“culturomics”, focusing on linguistic and cultural\nphenomena that were reflected in the English language\nbetween 1800 and 2000. We show how this approach can\nprovide insights about fields as diverse as lexicography,\nthe evolution of grammar, collective memory, the\nadoption of technology, the pursuit of fame, censorship,\nand historical epidemiology. “Culturomics” extends the\nboundaries of rigorous quantitative inquiry to a wide\narray of new phenomena spanning the social sciences and\nthe humanities.\nReading small collections of carefully chosen works enables\nscholars to make powerful inferences about trends in human\nthought. However, this approach rarely enables precise\nmeasurement of the underlying phenomena. Attempts to\nintroduce quantitative methods into the study of culture (1-6)\nhave been hampered by the lack of suitable data.\nWe report the creation of a corpus of 5,195,769 digitized\nbooks containing ~4% of all books ever published.\nComputational analysis of this corpus enables us to observe\ncultural trends and subject them to quantitative investigation.\n“Culturomics” extends the boundaries of scientific inquiry to\na wide array of new phenomena.\nThe corpus has emerged from Google’s effort to digitize\nbooks. Most books were drawn from over 40 university\nlibraries around the world. Each page was scanned with\ncustom equipment (7), and the text digitized using optical\ncharacter recognition (OCR). Additional volumes – both\nphysical and digital – were contributed by publishers.\nMetadata describing date and place of publication were\nprovided by the libraries and publishers, and supplemented\nwith bibliographic databases. Over 15 million books have\nbeen digitized [12% of all books ever published (7)]. We\nselected a subset of over 5 million books for analysis on the\nbasis of the quality of their OCR and metadata (Fig. 1A) (7).\nPeriodicals were excluded.\nThe resulting corpus contains over 500 billion words, in\nEnglish (361 billion), French (45B), Spanish (45B), German\n(37B), Chinese (13B), Russian (35B), and Hebrew (2B). The\noldest works were published in the 1500s. The early decades\nare represented by only a few books per year, comprising\nseveral hundred thousand words. By 1800, the corpus grows\nto 60 million words per year; by 1900, 1.4 billion; and by\n2000, 8 billion.\nThe corpus cannot be read by a human. If you tried to read\nonly the entries from the year 2000 alone, at the reasonable\npace of 200 words/minute, without interruptions for food or\nsleep, it would take eighty years. The sequence of letters is\none thousand times longer than the human genome: if you\nwrote it out in a straight line, it would reach to the moon and\nback 10 times over (8).\nTo make release of the data possible in light of copyright\nconstraints, we restricted our study to the question of how\noften a given “1-gram” or “n-gram” was used over time. A 1-\ngram is a string of characters uninterrupted by a space; this\nincludes words (“banana”, “SCUBA”) but also numbers\nDownloaded from www.sciencemag.org on December 16, 2010\n/ www.sciencexpress.org / 16 December 2010 / Page 1 / 10.1126/science.1199644\n(“3.14159”) and typos (“excesss”). An n-gram is sequence of\n1-grams, such as the phrases “stock market” (a 2-gram) and\n“the United States of America” (a 5-gram). We restricted n to\n5, and limited our study to n-grams occurring at least 40 times\nin the corpus.\nUsage frequency is computed by dividing the number of\ninstances of the n-gram in a given year by the total number of\nwords in the corpus in that year. For instance, in 1861, the 1-\ngram “slavery” appeared in the corpus 21,460 times, on\n11,687 pages of 1,208 books. The corpus contains\n386,434,758 words from 1861; thus the frequency is 5.5x10 -5 .\n“slavery” peaked during the civil war (early 1860s) and then\nagain during the civil rights movement (1955-1968) (Fig. 1B)\nIn contrast, we compare the frequency of “the Great War”\nto the frequencies of “World War I” and “World War II.” “the\nGreat War” peaks between 1915 and 1941. But although its\nfrequency drops thereafter, interest in the underlying events\nhad not disappeared; instead, they are referred to as “World\nWar I” (Fig. 1C).\nThese examples highlight two central factors that\ncontribute to culturomic trends. Cultural change guides the\nconcepts we discuss (such as “slavery”). Linguistic change –\nwhich, of course, has cultural roots – affects the words we use\nfor those concepts (“the Great War” vs. “World War I”). In\nthis paper, we will examine both linguistic changes, such as\nchanges in the lexicon and grammar; and cultural phenomena,\nsuch as how we remember people and events.\nThe full dataset, which comprises over two billion\nculturomic trajectories, is available for download or\nexploration at www.culturomics.org.\nThe Size of the English Lexicon\nHow many words are in the English language (9)?\nWe call a 1-gram “common” if its frequency is greater\nthan one per billion. (This corresponds to the frequency of the\nwords listed in leading dictionaries (7).) We compiled a list of\nall common 1-grams in 1900, 1950, and 2000 based on the\nfrequency of each 1-gram in the preceding decade. These lists\ncontained 1,117,997 common 1-grams in 1900, 1,102,920 in\n1950, and 1,489,337 in 2000.\nNot all common 1-grams are English words. Many fell\ninto three non-word categories: (i) 1-grams with nonalphabetic\ncharacters (“l8r”, “3.14159”); (ii) misspellings\n(“becuase, “abberation”); and (iii) foreign words\n(“sensitivo”).\nTo estimate the number of English words, we manually\nannotated random samples from the lists of common 1-grams\n(7) and determined what fraction were members of the above\nnon-word categories. The result ranged from 51% of all\ncommon 1-grams in 1900 to 31% in 2000.\nUsing this technique, we estimated the number of words in\nthe English lexicon as 544,000 in 1900, 597,000 in 1950, and\n1,022,000 in 2000. The lexicon is enjoying a period of\nenormous growth: the addition of ~8500 words/year has\nincreased the size of the language by over 70% during the last\nfifty years (Fig. 2A).\nNotably, we found more words than appear in any\ndictionary. For instance, the 2002 Webster’s Third New\nInternational Dictionary [W3], which keeps track of the\ncontemporary American lexicon, lists approximately 348,000\nsingle-word wordforms (10); the American Heritage\nDictionary of the English Language, Fourth Edition (AHD4)\nlists 116,161 (11). (Both contain additional multi-word\nentries.) Part of this gap is because dictionaries often exclude\nproper nouns and compound words (“whalewatching”). Even\naccounting for these factors, we found many undocumented\nwords, such as “aridification” (the process by which a\ngeographic region becomes dry), “slenthem” (a musical\ninstrument), and, appropriately, the word “deletable.”\nThis gap between dictionaries and the lexicon results from\na balance that every dictionary must strike: it must be\ncomprehensive enough to be a useful reference, but concise\nenough to be printed, shipped, and used. As such, many\ninfrequent words are omitted. To gauge how well dictionaries\nreflect the lexicon, we ordered our year 2000 lexicon by\nfrequency, divided it into eight deciles (ranging from 10 -9 –\n10 -8 to 10 -2 – 10 -1 ), and sampled each decile (7). We manually\nchecked how many sample words were listed in the OED (12)\nand in the Merriam-Webster Unabridged Dictionary [MWD].\n(We excluded proper nouns, since neither OED nor MWD\nlists them.) Both dictionaries had excellent coverage of high\nfrequency words, but less coverage for frequencies below 10 -\n6 : 67% of words in the 10 -9 – 10 -8 range were listed in neither\ndictionary (Fig. 2B). Consistent with Zipf’s famous law, a\nlarge fraction of the words in our lexicon (63%) were in this\nlowest frequency bin. As a result, we estimated that 52% of\nthe English lexicon – the majority of the words used in\nEnglish books – consists of lexical “dark matter”\nundocumented in standard references (12).\nTo keep up with the lexicon, dictionaries are updated\nregularly (13). We examined how well these changes\ncorresponded with changes in actual usage by studying the\n2077 1-gram headwords added to AHD4 in 2000. The overall\nfrequency of these words, such as “buckyball” and\n“netiquette”, has soared since 1950: two-thirds exhibited\nrecent, sharp increases in frequency (>2X from 1950-2000)\n(Fig. 2C). Nevertheless, there was a lag between\nlexicographers and the lexicon. Over half the words added to\nAHD4 were part of the English lexicon a century ago\n(frequency >10 -9 from 1890-1900). In fact, some newlyadded\nwords, such as “gypseous” and “amplidyne”, have\nalready undergone a steep decline in frequency (Fig. 2D).\nNot only must lexicographers avoid adding words that\nhave fallen out of fashion, they must also weed obsolete\nwords from earlier editions. This is an imperfect process. We\nDownloaded from www.sciencemag.org on December 16, 2010\n/ www.sciencexpress.org / 16 December 2010 / Page 2 / 10.1126/science.1199644\nfound 2220 obsolete 1-gram headwords (“diestock”,\n“alkalescent”) in AHD4. Their mean frequency declined\nthroughout the 20th century, and dipped below 10 -9 decades\nago (Fig. 2D, Inset).\nOur results suggest that culturomic tools will aid\nlexicographers in at least two ways: (i) finding low-frequency\nwords that they do not list; and (ii) providing accurate\nestimates of current frequency trends to reduce the lag\nbetween changes in the lexicon and changes in the dictionary.\nThe Evolution of Grammar\nNext, we examined grammatical trends. We studied the\nEnglish irregular verbs, a classic model of grammatical\nchange (14-17). Unlike regular verbs, whose past tense is\ngenerated by adding –ed (jump/jumped), irregulars are\nconjugated idiosyncratically (stick/stuck, come/came, get/got)\n(15).\nAll irregular verbs coexist with regular competitors (e.g.,\n“strived” and “strove”) that threaten to supplant them (Fig.\n2E). High-frequency irregulars, which are more readily\nremembered, hold their ground better. For instance, we found\n“found” (frequency: 5x10 -4 ) 200,000 times more often than\nwe finded “finded.” In contrast, “dwelt” (frequency: 1x10 -5 )\ndwelt in our data only 60 times as often as “dwelled” dwelled.\nWe defined a verb’s “regularity” as the percentage of\ninstances in the past tense (i.e., the sum of “drived”, “drove”,\nand “driven”) in which the regular form is used. Most\nirregulars have been stable for the last 200 years, but 16%\nunderwent a change in regularity of 10% or more (Fig. 2F).\nThese changes occurred slowly: it took 200 years for our\nfastest moving verb, “chide”, to go from 10% to 90%.\nOtherwise, each trajectory was sui generis; we observed no\ncharacteristic shape. For instance, a few verbs, like “spill”,\nregularized at a constant speed, but others, such as “thrive”\nand “dig”, transitioned in fits and starts (7). In some cases, the\ntrajectory suggested a reason for the trend. For example, with\n“sped/speeded” the shift in meaning from “to move rapidly”\nand towards “to exceed the legal limit” appears to have been\nthe driving cause (Fig. 2G).\nSix verbs (burn, chide, smell, spell, spill, thrive)\nregularized between 1800 and 2000 (Fig. 2F). Four are\nremnants of a now-defunct phonological process that used –t\ninstead of –ed; they are members of a pack of irregulars that\nsurvived by virtue of similarity (bend/bent, build/built,\nburn/burnt, learn/learnt, lend/lent, rend/rent, send/sent,\nsmell/smelt, spell/spelt, spill/spilt, and spoil/spoilt). Verbs\nhave been defecting from this coalition for centuries\n(wend/went, pen/pent, gird/girt, geld/gelt, and gild/gilt all\nblend/blent into the dominant –ed rule). Culturomic analysis\nreveals that the collapse of this alliance has been the most\nsignificant driver of regularization in the past 200 years. The\nregularization of burnt, smelt, spelt, and spilt originated in the\nUS; the forms still cling to life in British English (Fig. 2E,F).\nBut the –t irregulars may be doomed in England too: each\nyear, a population the size of Cambridge adopts “burned” in\nlieu of “burnt.”\nThough irregulars generally yield to regulars, two verbs\ndid the opposite: light/lit and wake/woke. Both were irregular\nin Middle English, were mostly regular by 1800, and\nsubsequently backtracked and are irregular again today. The\nfact that these verbs have been going back and forth for\nnearly 500 years highlights the gradual nature of the\nunderlying process.\nStill, there was at least one instance of rapid progress by\nan irregular form. Presently, 1% of the English speaking\npopulation switches from “sneaked” to “snuck” every year:\nsomeone will have snuck off while you read this sentence. As\nbefore, this trend is more prominent in the United States, but\nrecently sneaked across the Atlantic: America is the world’s\nleading exporter of both regular and irregular verbs.\nOut with the Old\nJust as individuals forget the past (18, 19), so do societies\n(20). To quantify this effect, we reasoned that the frequency\nof 1-grams such as “1951” could be used to measure interest\nin the events of the corresponding year, and created plots for\neach year between 1875 and 1975.\nThe plots had a characteristic shape. For example, “1951”\nwas rarely discussed until the years immediately preceding\n1951. Its frequency soared in 1951, remained high for three\nyears, and then underwent a rapid decay, dropping by half\nover the next fifteen years. Finally, the plots enter a regime\nmarked by slower forgetting: collective memory has both a\nshort-term and a long-term component.\nBut there have been changes. The amplitude of the plots is\nrising every year: precise dates are increasingly common.\nThere is also a greater focus on the present. For instance,\n“1880” declined to half its peak value in 1912, a lag of 32\nyears. In contrast, “1973” declined to half its peak by 1983, a\nlag of only 10 years. We are forgetting our past faster with\neach passing year (Fig. 3A).\nWe were curious whether our increasing tendency to forget\nthe old was accompanied by more rapid assimilation of the\nnew (21). We divided a list of 154 inventions into timeresolved\ncohorts based on the forty-year interval in which\nthey were first invented (1800-1840, 1840-1880, and 1880-\n1920) (7). We tracked the frequency of each invention in the\nnth after it was invented as compared to its maximum value,\nand plotted the median of these rescaled trajectories for each\ncohort.\nThe inventions from the earliest cohort (1800-1840) took\nover 66 years from invention to widespread impact\n(frequency >25% of peak). Since then, the cultural adoption\nof technology has become more rapid: the 1840-1880\ninvention cohort was widely adopted within 50 years; the\n1880-1920 cohort within 27 (Fig. 3B).\nDownloaded from www.sciencemag.org on December 16, 2010\n/ www.sciencexpress.org / 16 December 2010 / Page 3 / 10.1126/science.1199644\n“In the Future, Everyone Will Be World Famous for 7.5\nMinutes” –Whatshisname\nPeople, too, rise to prominence, only to be forgotten (22).\nFame can be tracked by measuring the frequency of a\nperson’s name (Fig. 3C). We compared the rise to fame of the\nmost famous people of different eras. We took all 740,000\npeople with entries in Wikipedia, removed cases where\nseveral famous individuals share a name, and sorted the rest\nby birthdate and frequency (23). For every year from 1800-\n1950, we constructed a cohort consisting of the fifty most\nfamous people born in that year. For example, the 1882\ncohort includes “Virginia Woolf” and “Felix Frankfurter”; the\n1946 cohort includes “Bill Clinton” and “Steven Spielberg.”\nWe plotted the median frequency for the names in each\ncohort over time (Fig. 3D-E). The resulting trajectories were\nall similar. Each cohort had a pre-celebrity period ( median\nfrequency <10 -9 ), followed by a rapid rise to prominence, a\npeak, and a slow decline. We therefore characterized each\ncohort using four parameters: (i) the age of initial celebrity;\n(ii) the doubling time of the initial rise; (iii) the age of peak\ncelebrity; (iv) the half-life of the decline (Fig. 3E). The age of\npeak celebrity has been consistent over time: about 75 years\nafter birth. But the other parameters have been changing.\nFame comes sooner and rises faster: between the early 19th\ncentury and the mid-20th century, the age of initial celebrity\ndeclined from 43 to 29 years, and the doubling time fell from\n8.1 to 3.3 years. As a result, the most famous people alive\ntoday are more famous – in books – than their predecessors.\nYet this fame is increasingly short-lived: the post-peak halflife\ndropped from 120 to 71 years during the nineteenth\ncentury.\nWe repeated this analysis with all 42,358 people in the\ndatabases of Encyclopaedia Britannica (24), which reflect a\nprocess of expert curation that began in 1768. The results\nwere similar (7). Thus, people are getting more famous than\never before, but are being forgotten more rapidly than ever.\nOccupational choices affect the rise to fame. We focused\non the 25 most famous individuals born between 1800 and\n1920 in seven occupations (actors, artists, writers, politicians,\nbiologists, physicists, and mathematicians), examining how\ntheir fame grew as a function of age (Fig. 3F).\nActors tend to become famous earliest, at around 30. But\nthe fame of the actors we studied – whose ascent preceded the\nspread of television – rises slowly thereafter. (Their fame\npeaked at a frequency of 2x10 -7 .) The writers became famous\nabout a decade after the actors, but rose for longer and to a\nmuch higher peak (8x10 -7 ). Politicians did not become\nfamous until their 50s, when, upon being elected President of\nthe United States (in 11 of 25 cases; 9 more were heads of\nother states) they rapidly rose to become the most famous of\nthe groups (1x10 -6 ).\nScience is a poor route to fame. Physicists and biologists\neventually reached a similar level of fame as actors (1x10 -7 ),\nbut it took them far longer. Alas, even at their peak,\nmathematicians tend not to be appreciated by the public\n(2x10 -8 ).\nDetecting Censorship and Suppression\nSuppression – of a person, or an idea – leaves quantifiable\nfingerprints (25). For instance, Nazi censorship of the Jewish\nartist Marc Chagall is evident by comparing the frequency of\n“Marc Chagall” in English and in German books (Fig.4A). In\nboth languages, there is a rapid ascent starting in the late\n1910s (when Chagall was in his early 30s). In English, the\nascent continues. But in German, the artist’s popularity\ndecreases, reaching a nadir from 1936-1944, when his full\nname appears only once. (In contrast, from 1946-1954, “Marc\nChagall” appears nearly 100 times in the German corpus.)\nSuch examples are found in many countries, including Russia\n(e.g. Trotsky), China (Tiananmen Square) and the US (the\nHollywood Ten, blacklisted in 1947) (Fig.4B-D).\nWe probed the impact of censorship on a person’s cultural\ninfluence in Nazi Germany. Led by such figures as the\nlibrarian Wolfgang Hermann, the Nazis created lists of\nauthors and artists whose “undesirable”, “degenerate” work\nwas banned from libraries and museums and publicly burned\n(26-28). We plotted median usage in German for five such\nlists: artists (100 names), as well as writers of Literature\n(147), Politics (117), History (53), and Philosophy (35) (Fig\n4E). We also included a collection of Nazi party members\n[547 names, ref (7)]. The five suppressed groups exhibited a\ndecline. This decline was modest for writers of history (9%)\nand literature (27%), but pronounced in politics (60%),\nphilosophy (76%), and art (56%). The only group whose\nsignal increased during the Third Reich was the Nazi party\nmembers [a 500% increase; ref (7)].\nGiven such strong signals, we tested whether one could\nidentify victims of Nazi repression de novo. We computed a\n“suppression index” s for each person by dividing their\nfrequency from 1933 – 1945 by the mean frequency in 1925-\n1933 and in 1955-1965 (Fig.4F, Inset). In English, the\ndistribution of suppression indices is tightly centered around\nunity. Fewer than 1% of individuals lie at the extremes (s<1/5\nor s>5).\nIn German, the distribution in much wider, and skewed\nleftward: suppression in Nazi Germany was not the\nexception, but the rule (Fig. 4F). At the far left, 9.8% of\nindividuals showed strong suppression (s<1/5). This\npopulation is highly enriched for documented victims of\nrepression, such as Pablo Picasso (s=0.12), the Bauhaus\narchitect Walter Gropius (s=0.16), and Hermann Maas\n(s<.01), an influential Protestant Minister who helped many\nJews flee (7). (Maas was later recognized by Israel’s Yad\nVashem as a “Righteous Among the Nations.”) At the other\nDownloaded from www.sciencemag.org on December 16, 2010\n/ www.sciencexpress.org / 16 December 2010 / Page 4 / 10.1126/science.1199644\nextreme, 1.5% of the population exhibited a dramatic rise\n(s>5). This subpopulation is highly enriched for Nazis and\nNazi-supporters, who benefited immensely from government\npropaganda (7).\nThese results provide a strategy for rapidly identifying\nlikely victims of censorship from a large pool of possibilities,\nand highlights how culturomic methods might complement\nexisting historical approaches.\nCulturomics\nCulturomics is the application of high-throughput data\ncollection and analysis to the study of human culture. Books\nare a beginning, but we must also incorporate newspapers\n(29), manuscripts (30), maps (31), artwork (32), and a myriad\nof other human creations (33, 34). Of course, many voices –\nalready lost to time – lie forever beyond our reach.\nCulturomic results are a new type of evidence in the\nhumanities. As with fossils of ancient creatures, the challenge\nof culturomics lies in the interpretation of this evidence.\nConsiderations of space restrict us to the briefest of surveys: a\nhandful of trajectories and our initial interpretations. Many\nmore fossils, with shapes no less intriguing, beckon:\n(i) Peaks in “influenza” correspond with dates of known\npandemics, suggesting the value of culturomic methods for\nhistorical epidemiology (35) (Fig. 5A).\n(ii) Trajectories for “the North”, “the South”, and finally,\n“the enemy” reflect how polarization of the states preceded\nthe descent into war (Fig. 5B).\n(iii) In the battle of the sexes, the “women” are gaining\nground on the “men” (Fig. 5C).\n(iv) “féminisme” made early inroads in France, but the US\nproved to be a more fertile environment in the long run (Fig.\n5D).\n(v) “Galileo”, “Darwin”, and “Einstein” may be well-known\nscientists, but “Freud” is more deeply engrained in our\ncollective subconscious (Fig. 5E).\n(vi) Interest in “evolution” was waning when “DNA”\ncame along (Fig. 5F).\n(vii) The history of the American diet offers many\nappetizing opportunities for future research; the menu\nincludes “steak”, “sausage”, “ice cream”, “hamburger”,\n“pizza”, “pasta”, and “sushi” (Fig. 5G).\n(viii) “God” is not dead; but needs a new publicist (Fig.\n5H).\nThese, together with the billions of other trajectories that\naccompany them, will furnish a great cache of bones from\nwhich to reconstruct the skeleton of a new science.\nReferences and Notes\n1. Wilson, Edward O. Consilience. New York: Knopf, 1998.\n2. Sperber, Dan. \"Anthropology and psychology: Towards an\nepidemiology of representations.\" Man 20 (1985): 73-89.\n3. Lieberson, Stanley and Joel Horwich. \"Implication\nanalysis: a pragmatic proposal for linking theory and data\nin the social sciences.\" Sociological Methodology 38\n(December 2008): 1-50.\n4. Cavalli-Sforza, L. L., and Marcus W. Feldman. Cultural\nTransmission and Evolution. Princeton, NJ: Princeton UP,\n1981.\n5. Niyogi, Partha. The Computational Nature of Language\nLearning and Evolution. Cambridge, MA: MIT, 2006.\n6. Zipf, George Kingsley. The Psycho-biology of Language.\nBoston: Houghton Mifflin, 1935.\n7. Materials and methods are available as supporting material\non Science Online.\n8. Lander, E. S. et al. \"Initial sequencing and analysis of the\nhuman genome.\" Nature 409 (February 2001): 860-921.\n9. Read, Allen W. “The Scope of the American Dictionary.”\nAmerican Speech 8 (1933): 10–20.\n10. Gove, Philip Babcock, ed. Webster's Third New\nInternational Dictionary of the English Language,\nUnabridged. Springfield, MA: Merriam-Webster, 1993.\n11. Pickett, Joseph, P. ed. The American Heritage Dictionary\nof the English Language, Fourth Edition. Boston / New\nYork, NY: Houghton Mifflin Pub., 2000.\n12. Simpson, J. A., E. S. C. Weiner, and Michael Proffitt, eds.\nOxford English Dictionary. Oxford [England]: Clarendon,\n1993.\n13. Algeo, John, and Adele S. Algeo. Fifty Years among the\nNew Words: a Dictionary of Neologisms, 1941-1991.\nCambridge UK, 1991.\n14. Pinker, Steven. Words and Rules. New York: Basic,\n1999.\n15. Kroch, Anthony S. \"Reflexes of Grammar in Patterns of\nLanguage Change.\" Language Variation and Change 1.03\n(1989): 199.\n16. Bybee, Joan L. \"From Usage to Grammar: The Mind's\nResponse to Repetition.\" Language 82.4 (2006): 711-33.\n17. Lieberman*, Erez, Jean-Baptiste Michel*, Joe Jackson,\nTina Tang, and Martin A. Nowak. \"Quantifying the\nEvolutionary Dynamics of Language.\" Nature 449 (2007):\n713-16.\n18. Milner, Brenda, Larry R. Squire, and Eric R. Kandel.\n\"Cognitive Neuroscience and the Study of\nMemory.\"Neuron 20.3 (1998): 445-68.\n19. Ebbinghaus, Hermann. Memory: a Contribution to\nExperimental Psychology. New York: Dover, 1987.\n20. Halbwachs, Maurice. On Collective Memory. Trans.\nLewis A. Coser. Chicago: University of Chicago, 1992.\n21. Ulam, S. \"John Von Neumann 1903-1957.\" Bulletin of\nthe American Mathematical Society 64.3 (1958): 1-50.\n22. Braudy, Leo. The Frenzy of Renown: Fame & Its History.\nNew York: Vintage, 1997.\nDownloaded from www.sciencemag.org on December 16, 2010\n/ www.sciencexpress.org / 16 December 2010 / Page 5 / 10.1126/science.1199644\n23. Wikipedia. Web. 23 Aug. 2010.\n<http://www.wikipedia.org/>.\n24. Hoiberg, Dale, ed. Encyclopaedia Britannica. Chicago:\nEncyclopaedia Britannica, 2002.\n25. Gregorian, Vartan, ed. Censorship: 500 Years of Conflict.\nNew York: New York Public Library, 1984.\n26. Treß, Werner. Wider Den Undeutschen Geist:\nBücherverbrennung 1933. Berlin: Parthas, 2003.\n27. Sauder, Gerhard. Die Bücherverbrennung: 10. Mai 1933.\nFrankfurt/Main: Ullstein, 1985.\n28. Barron, Stephanie, and Peter W. Guenther. Degenerate\nArt: the Fate of the Avant-garde in Nazi Germany. Los\nAngeles: Los Angeles County Museum of Art, 1991.\n29. Google News Archive Search. Web.\n<http://news.google.com/archivesearch>.\n30. Digital Scriptorium. Web.\n<http://www.scriptorium.columbia.edu>.\n31. Visual Eyes. Web. <http://www.viseyes.org>.\n32. ARTstor. Web. <http://www.artstor.org>.\n33. Europeana. Web. <http://www.europeana.eu>.\n34. Hathi Trust Digital Library. Web.\n<http://www.hathitrust.org>.\n35. Barry, John M. The Great Influenza: the Epic Story of the\nDeadliest Plague in History. New York: Viking, 2004.\n36. J-B.M. was supported by the Foundational Questions in\nEvolutionary Biology Prize Fellowship and the Systems\nBiology Program (Harvard Medical School). Y.K.S. was\nsupported by internships at Google. S.P. acknowledges\nsupport from NIH grant HD 18381. E.A. was supported by\nthe Harvard Society of Fellows, the Fannie and John Hertz\nFoundation Graduate Fellowship, the National Defense\nScience and Engineering Graduate Fellowship, the NSF\nGraduate Fellowship, the National Space Biomedical\nResearch Institute, and NHGRI Grant T32 HG002295 .\nThis work was supported by a Google Research Award.\nThe Program for Evolutionary Dynamics acknowledges\nsupport from the Templeton Foundation, NIH grant\nR01GM078986, and the Bill and Melinda Gates\nFoundation. Some of the methods described in this paper\nare covered by US patents 7463772 and 7508978. We are\ngrateful to D. Bloomberg, A. Popat, M. McCormick, T.\nMitchison, U. Alon, S. Shieber, E. Lander, R. Nagpal, J.\nFruchter, J. Guldi, J. Cauz, C. Cole, P. Bordalo, N.\nChristakis, C. Rosenberg, M. Liberman, J. Sheidlower, B.\nZimmer, R. Darnton, and A. Spector for discussions; to C-\nM. Hetrea and K. Sen for assistance with Encyclopaedia\nBritannica's database, to S. Eismann, W. Treß, and the\nCity of Berlin website (berlin.de) for assistance\ndocumenting victims of Nazi censorship, to C. Lazell and\nG.T. Fournier for assistance with annotation, to M. Lopez\nfor assistance with Fig. 1, to G. Elbaz and W. Gilbert for\nreviewing an early draft, and to Google’s library partners\nand every author who has ever picked up a pen, for books.\nSupporting Online Material\nwww.sciencemag.org/cgi/content/full/science.1199644/DC1\nMaterials and Methods\nFigs. S1 to S19\nReferences\n27 October 2010; accepted 6 December 2010\nPublished online 16 December 2010;\n10.1126/science.1199644\nFig. 1. “Culturomic” analyses study millions of books at\nonce. (A) Top row: authors have been writing for millennia;\n~129 million book editions have been published since the\nadvent of the printing press (upper left). Second row:\nLibraries and publishing houses provide books to Google for\nscanning (middle left). Over 15 million books have been\ndigitized. Third row: each book is associated with metadata.\nFive million books are chosen for computational analysis\n(bottom left). Bottom row: a culturomic “timeline” shows the\nfrequency of “apple” in English books over time (1800-\n2000). (B) Usage frequency of “slavery.” The Civil War\n(1861-1865) and the civil rights movement (1955-1968) are\nhighlighted in red. The number in the upper left (1e-4) is the\nunit of frequency. (C) Usage frequency over time for “the\nGreat War” (blue), “World War I” (green), and “World War\nII” (red).\nFig. 2. Culturomics has profound consequences for the study\nof language, lexicography, and grammar. (A) The size of the\nEnglish lexicon over time. Tick marks show the number of\nsingle words in three dictionaries (see text). (B) Fraction of\nwords in the lexicon that appear in two different dictionaries\nas a function of usage frequency. (C) Five words added by\nthe AHD in its 2000 update. Inset: Median frequency of new\nwords added to AHD4 in 2000. The frequency of half of these\nwords exceeded 10 -9 as far back as 1890 (white dot). (D)\nObsolete words added to AHD4 in 2000. Inset: Mean\nfrequency of the 2220 AHD headwords whose current usage\nfrequency is less than 10 -9 . (E) Usage frequency of irregular\nverbs (red) and their regular counterparts (blue). Some verbs\n(chide/chided) have regularized during the last two centuries.\nThe trajectories for “speeded” and “speed up” (green) are\nsimilar, reflecting the role of semantic factors in this instance\nof regularization. The verb “burn” first regularized in the US\n(US flag) and later in the UK (UK flag). The irregular\n“snuck” is rapidly gaining on “sneaked.” (F) Scatter plot of\nthe irregular verbs; each verb’s position depends on its\nregularity (see text) in the early 19th century (x-coordinate)\nand in the late 20th century (y-coordinate). For 16% of the\nverbs, the change in regularity was greater than 10% (large\nfont). Dashed lines separate irregular verbs (regularity<50%)\nDownloaded from www.sciencemag.org on December 16, 2010\n/ www.sciencexpress.org / 16 December 2010 / Page 6 / 10.1126/science.1199644\nfrom regular verbs (regularity>50%). Six verbs became\nregular (upper left quadrant, blue), while two became\nirregular (lower right quadrant, red). Inset: the regularity of\n“chide” over time. (G) Median regularity of verbs whose past\ntense is often signified with a –t suffix instead of –ed (burn,\nsmell, spell, spill, dwell, learn, and spoil) in US (black) and\nUK (grey) books.\nFig. 3. Cultural turnover is accelerating. (A) We forget:\nfrequency of 1883 (blue), 1910 (green) and 1950 (red). Inset:\nWe forget faster. The half-life of the curves (grey dots) is\ngetting shorter (grey line: moving average). (B) Cultural\nadoption occurs faster. Median trajectory for three cohorts of\ninventions from three different time periods (1800-1840:\nblue, 1840-1880: green, 1880-1920: red). Inset: The\ntelephone (green, date of invention: green arrow) and radio\n(blue, date of invention: blue arrow). (C) Fame of various\npersonalities born between 1920 and 1930. (D) Frequency of\nthe 50 most famous people born in 1871 (grey lines; median:\ndark gray). Five examples are highlighted. (E) The median\ntrajectory of the 1865 cohort is characterized by four\nparameters: (i) initial “age of celebrity” (34 years old, tick\nmark); (ii) doubling time of the subsequent rise to fame (4\nyears, blue line); (iii) “age of peak celebrity” (70 years after\nbirth, tick mark), and (iv) half-life of the post-peak\n“forgetting” phase (73 years, red line). Inset: The doubling\ntime and half-life over time. (F) The median trajectory of the\n25 most famous personalities born between 1800 and 1920 in\nvarious careers.\nFig. 4. Culturomics can be used to detect censorship. (A)\nUsage frequency of “Marc Chagall” in German (red) as\ncompared to English (blue). (B) Suppression of Leon Trotsky\n(blue), Grigory Zinoviev (green), and Lev Kamenev (red) in\nRussian texts, with noteworthy events indicated: Trotsky’s\nassassination (blue arrow), Zinoviev and Kamenev executed\n(red arrow), the “Great Purge” (red highlight), perestroika\n(grey arrow). (C) The 1976 and 1989 Tiananmen Square\nincidents both lead to elevated discussion in English texts.\nResponse to the 1989 incident is largely absent in Chinese\ntexts (blue), suggesting government censorship. (D) After the\n“Hollywood Ten” were blacklisted (red highlight) from\nAmerican movie studios, their fame declined (median: wide\ngrey). None of them were credited in a film until 1960’s\n(aptly named) “Exodus.” (E) Writers in various disciplines\nwere suppressed by the Nazi regime (red highlight). In\ncontrast, the Nazis themselves (thick red) exhibited a strong\nfame peak during the war years. (F) Distribution of\nsuppression indices for both English (blue) and German (red)\nfor the period from 1933-1945. Three victims of Nazi\nsuppression are highlighted at left (red arrows). Inset:\nCalculation of the suppression index for “Henri Matisse.”\nFig. 5. Culturomics provides quantitative evidence for\nscholars in many fields. (A) Historical Epidemiology:\n“influenza” is shown in blue; the Russian, Spanish, and Asian\nflu epidemics are highlighted. (B) History of the Civil War.\n(C) Comparative History. (D) Gender studies. (E and F)\nHistory of Science. (G) Historical Gastronomy. (H) History\nof Religion: “God.”\nDownloaded from www.sciencemag.org on December 16, 2010\n/ www.sciencexpress.org / 16 December 2010 / Page 7 / 10.1126/science.1199644\n\n\n\n\n\nwww.sciencemag.org/cgi/content/full/science.1199644/DC1\nSupporting Online Material for\nQuantitative Analysis of Culture Using Millions of Digitized Books\nJean-Baptiste Michel,* Yuan Kui Shen, Aviva Presser Aiden, Adrian Veres, Matthew K.\nGray, The Google Books Team, Joseph P. Pickett, Dale Hoiberg, Dan Clancy, Peter\nNorvig, Jon Orwant, Steven Pinker, Martin A. Nowak, Erez Lieberman Aiden*\n*To whom correspondence should be addressed. E-mail: jb.michel@gmail.com (J.B.M.); erez@erez.com\n(E.A.).\nThis PDF file includes:\nMaterials and Methods\nFigs. S1 to S19\nReferences\nPublished 16 December 2010 on Science Express\nDOI: 10.1126/science.1199644\nMaterials and Methods\n“Quantitative analysis of culture using millions of digitized books”,\nMichel et al.\nContents\nI. Overview of Google Books Digitization ......................................................................................... 3\nI.1. Metadata ....................................................................................................................... 3\nI.2. Digitization ..................................................................................................................... 4\nI.3. Structure Extraction ...................................................................................................... 4\nII. Construction of Historical N-grams Corpora ................................................................................ 5\nII.1. Additional filtering of books .......................................................................................... 5\nII.1A. Accuracy of Date-of-Publication metadata ................................................................ 5\nII.1B. OCR quality ............................................................................................................... 6\nII.1C. Accuracy of language metadata ................................................................................ 6\nII.1D. Year Restriction ......................................................................................................... 7\nII.2. Metadata based subdivision of the Google Books Collection ...................................... 7\nII.2A. Determination of language ........................................................................................ 7\nII.2B. Determination of book subject assignments .............................................................. 7\nII.2C. Determination of book country-of-publication ............................................................ 7\nII.3. Construction of historical n-grams corpora .................................................................. 8\nII.3A. Creation of a digital sequence of 1-grams and extraction of n-gram counts ............. 8\nII.3B. Generation of historical n-grams corpora ................................................................ 10\nIII. Culturomic Analyses .............................................................................................................................. 12\nIII.0. General Remarks ................................................................................................................... 12\nIII.0.1 On Corpora. ......................................................................................................................... 12\nIII.0.2 On the number of books published ...................................................................................... 13\nIII.1. Generation of timeline plots ................................................................................................... 13\nIII.1A. Single Query ........................................................................................................................ 13\nIII.1B. Multiple Query/Cohort Timelines ......................................................................................... 14\nIII.2. Note on collection of historical and cultural data ................................................................... 14\nIII.3. Controls .................................................................................................................................. 15\nIII.4. Lexicon Analysis .................................................................................................................... 15\n1\nIII.4A. Estimation of the number of 1-grams defined in leading dictionaries of the English\nlanguage. ....................................................................................................................................... 15\nIII.4B. Estimation of Lexicon Size .................................................................................................. 16\nIII.4C. Dictionary Coverage ............................................................................................................ 17\nIII.4D. Analysis New and Obsolete words in the American Heritage Dictionary ............................ 17\nIII.5. The Evolution of Grammar ..................................................................................................... 17\nIII.5A. Ensemble of verbs studied .................................................................................................. 17\nIII.5B. Verb frequencies.................................................................................................................. 18\nIII.5C. Rates of regularization ........................................................................................................ 18\nIII.5D. Classification of Verbs ......................................................................................................... 18\nIII.6. Collective Memory.................................................................................................................. 18\nIII.7. The Pursuit of Fame............................................................................................................... 19\nIII.7A) Complete procedure ............................................................................................................ 19\nIII.7B. Cohorts of fame ................................................................................................................... 25\nIII.8. History of Technology ............................................................................................................ 26\nIII.9. Censorship ............................................................................................................................. 26\nIII.9A. Comparing the influence of censorship and propaganda on various groups ...................... 26\nIII.9B. De Novo Identification of Censored and Suppressed Individuals ....................................... 28\nIII.9C. Validation by an expert annotator........................................................................................ 28\nIII.10. Epidemics ............................................................................................................................. 29\n2\nI. Overview of Google Books Digitization\nIn 2004, Google began scanning books to make their contents searchable and discoverable online. To\ndate, Google has scanned over fifteen million books: over 11% of all the books ever published. The\ncollection contains over five billion pages and two trillion words, with books dating back to as early as\n1473 and with text in 478 languages. Over two million of these scanned books were given directly to\nGoogle by their publishers; the rest are borrowed from large libraries such as the University of Michigan\nand the New York Public Library. The scanning effort involves significant engineering challenges, some of\nwhich are highly relevant to the construction of the historical n-grams corpus. We survey those issues\nhere.\nThe result of the next three steps is a collection of digital texts associated with particular book editions, as\nwell as composite metadata for each edition combining the information contained in all metadata sources.\nI.1. Metadata\nOver 100 sources of metadata information were used by Google to generate a comprehensive catalog of\nbooks. Some of these sources are library catalogs (e.g., the list of books in the collections of University of\nMichigan, or union catalogs such as the collective list of books in Bosnian libraries), some are from\nretailers (e.g., Decitre, a French bookseller), and some are from commercial aggregators (e.g., Ingram).\nIn addition, Google also receives metadata from its 30,000 partner publishers. Each metadata source\nconsists of a series of digital records, typically in either the MARC format favored by libraries, or the ONIX\nformat used by the publishing industry. Each record refers to either a specific edition of a book or a\nphysical copy of a book on a library shelf, and contains conventional bibliographic data such as title,\nauthor(s), publisher, date of publication, and language(s) of publication.\nCataloguing practices vary widely among these sources, and even within a single source over time. Thus\ntwo records for the same edition will often differ in multiple fields. This is especially true for serials (e.g.,\nthe Congressional Record) and multivolume works such as sets (e.g., the three volumes of The Lord of\nthe Rings).\nThe matter is further complicated by ambiguities in the definition of the word „book‟ itself. Including\ntranslations, there are over three thousand editions derived from Mark Twain‟s original Tom Sawyer.\nGoogle‟s process of converting the billions of metadata records into a single nonredundant database of\nbook editions consists of the following principal steps:\n3\n1. Coarsely dividing the billions of metadata records into groups that may refer to the same\nwork (e.g., Tom Sawyer).\n2. Identifying and aggregating multivolume works based on the presence of cues from individual\nrecords.\n3. Subdividing the group of records corresponding to each work into constituent groups\ncorresponding to the various editions (e.g., the 1909 publication of De lotgevallen van Tom\nSawyer, translated from English to Dutch by Johan Braakensiek).\n4. Merging the records for each edition into a new “consensus” record.\nThe result is a set of consensus records, where each record corresponds to a distinct book edition and\nwork, and where the contents of each record are formed out of fields from multiple sources. The number\nof records in this set -- i.e., the number of known book editions -- increases every year as more books are\nwritten.\nIn August 2010, this evaluation identified 129 million editions, which is the working estimate we use in this\npaper of all the editions ever published (this includes serials and sets but excludes kits, mixed media, and\nperiodicals such as newspapers). This final database contains bibliographic information for each of these\n129 million editions (Ref. S1). The country of publication is known for 85.3% of these editions, authors for\n87.8%, publication dates for 92.6%, and the language for 91.6%. Of the 15 million books scanned, the\ncountry of publication is known for 91.5%, authors for 92.1%, publication dates for 95.1%, and the\nlanguage for 98.6%.\nI.2. Digitization\nWe describe the way books are scanned and digitized. For publisher-provided books, Google removes\nthe spines and scans the pages with industrial sheet-fed scanners. For library-provided books, Google\nuses custom-built scanning stations designed to impose only as much wear on the book as would result\nfrom someone reading the book. As the pages are turned, stereo cameras overhead photograph each\npage, as shown in Figure S1.\nOne crucial difference between sheet-fed scanners and the stereo scanning process is the flatness of the\npage as the image is captured. In sheet-fed scanning, the page is kept flat, similar to conventional flatbed\nscanners. With stereo scanning, the book is cradled at an angle that minimizes stress on the spine of the\nbook (this angle is not shown in Figure S1). Though less damaging to the book, a disadvantage of the\nlatter approach is that it results in a page that is curved relative to the plane of the camera. The curvature\nchanges every time a page is turned, for several reasons: the attachment point of the page in the spine\ndiffers, the two stacks of pages change in thickness, and the tension with which the book is held open\nmay vary. Thicker books have more page curvature and more variation in curvature.\nThis curvature is measured by projecting a fixed infrared pattern onto each page of the book,\nsubsequently captured by cameras. When the image is later processed, this pattern is used to identify the\nlocation of the spine and to determine the curvature of the page. Using this curvature information, the\nscanned image of each page is digitally resampled so that the results correspond as closely as possible\nto the results of sheet-fed scanning. The raw images are also digitally cropped, cleaned, and contrast\nenhanced. Blurred pages are automatically detected and rescanned. Details of this approach can be\nfound in U.S. Patents 7463772 and 7508978; sample results are shown in Figure S2.\nFinally, blocks of text are identified and optical character recognition (OCR) is used to convert those\nimages into digital characters and words, in an approach described elsewhere (Ref. S2). The difficulty of\napplying conventional OCR techniques to Google‟s scanning effort is compounded because of variations\nin language, font, size, paper quality, and the physical condition of the books being scanned.\nNevertheless, Google estimates that over 98% of words are correctly digitized for modern English books.\nAfter OCR, initial and trailing punctuation is stripped and word fragments split by hyphens are joined,\nyielding a stream of words suitable for subsequent indexing.\nI.3. Structure Extraction\nAfter the book has been scanned and digitized, the components of the scanned material are classified\ninto various types. For instance, individual pages are scanned in order to identify which pages comprise\nthe authored content of the book, as opposed to the pages which comprise frontmatter and backmatter,\nsuch as copyright pages, tables of contents, index pages, etc. Within each page, we also identify\nrepeated structural elements, such as headers, footers, and page numbers.\nUsing OCR results from the frontmatter and backmatter, we automatically extract author names, titles,\nISBNs, and other identifying information. This information is used to confirm that the correct consensus\nrecord has been associated with the scanned text.\n4\nII. Construction of Historical N-grams Corpora\nAs noted in the paper text, we did not analyze the entire set of 15 million books digitized by Google.\nInstead, we\n1. Performed further filtering steps to select only a subset of books with highly accurate metadata.\n2. Subdivided the books into „base corpora‟ using such metadata fields as language, country of\npublication, and subject.\n3. For each base corpus, construct a massive numerical table that lists, for each n-gram (often a\nword or phrase), how often it appears in the given base corpus in every single year between 1550\nand 2008.\nIn this section, we will describe these three steps. These additional steps ensure high data quality, and\nalso make it possible to examine historical trends without violating the 'fair use' principle of copyright law:\nour object of study is the frequency tables produced in step 3 (which are available as supplemental data),\nand not the full-text of the books.\nII.1. Additional filtering of books\nII.1A. Accuracy of Date-of-Publication metadata\nAccurate date-of-publication data is crucial component in the production of time-resolved n-grams data.\nBecause our study focused most centrally on the English language corpus, we decided to apply more\nstringent inclusion criteria in order to make sure the accuracy of the date-of-publication data was as high\nas possible.\nWe found that the lion's share of date-of-publication errors were due to so-called 'bound-withs' - single\nvolumes that contain multiple works, such as anthologies or collected works of a given author. Among\nthese bound-withs, the most inaccurately dated subclass were serial publications, such as journals and\nperiodicals. For instance, many journals had publication dates which were erroneously attributed to the\nyear in which the first issue of the journal had been published. These journals and serial publications also\nrepresented a different aspect of culture than the books did. For these reasons, we decided to filter out all\nserial publications to the extent possible. Our 'Serial Killer' algorithm removed serial publications by\nlooking for suggestive metadata entries, containing one or more of the following:\n5\n1. Serial-associated titles, containing such phrases as 'Journal of', 'US Government report', etc.\n2. Serial-associated authors, such as those in which the author field is blank, too numerous, or\ncontains words such as 'committee'.\nNote that the match is case-insensitive, and it must be to a complete word in the title; thus the filtering of\ntitles containing the word „digest‟ does not lead to the removal of works with „digestion‟ in the title. The\nentire list of serial-associated title phrases and serial-associated author phrases is included as\nsupplemental data (Appendix). For English books, 29.4% of books were filtered using the 'Serial Killer',\nwith the title filter removing 2% and the author filter removing 27.4%. Foreign language corpora were\nfiltered in a similar fashion.\nThis filtering step markedly increased the accuracy of the metadata dates. We determined metadata\naccuracy by examining 1000 filtered volumes distributed uniformly over time from 1801-2000 (5 per year).\nAn annotator with no knowledge of our study manually determined the date-of-publication. The annotator\nwas aware of the Google metadata dates during this process. We found that 5.8% of English books had\nmetadata dates that were more than 5 years from the date determined by a human examining the book.\nBecause errors are much more common among older books, and because the actual corpora are strongly\nbiased toward recent works, the likelihood of error in a randomly sampled book from the final corpus is\nmuch lower than 6.2%. As a point of comparison, 27 of 100 books (27%) selected at random from an\nunfiltered corpus contained date-of-publication errors of greater than 5 years. The unfiltered corpus was\ncreated using a sampling strategy similar to that of Eng-1M. This selection mechanism favored recent\nbooks (which are more frequent) and pre-1800 books, which were excluded in the sampling strategy for\nfiltered books; as such the two numbers (6.2% and 27%) give a sense of the improvement, but are not\nstrictly comparable.\nNote that since the base corpora were generated (August 2009), many additional improvements have\nbeen made to the metadata dates used in Google Book Search itself. As such, these numbers do not\nreflect the accuracy of the Google Book Search online tool.\nII.1B. OCR quality\nThe challenge of performing accurate OCR on the entire books dataset is compounded by variations in\nsuch factors as language, font, size, legibility, and physical condition of the book. OCR quality was\nassessed using an algorithm developed by Popat et al. (Ref S3). This algorithm yields a probability that\nexpresses the confidence that a given sequence of text generated by OCR is correct. Incorrect or\nanomalous text can result from gross imperfections in the scanned images, or as a result of markings or\ndrawings. This algorithm uses sophisticated statistics, a variant of the Partial by Partial Matching (PPM)\nmodel, to compute for each glyph (character) the probability that it is anomalous given other nearby\nglyphs. ('Nearby' refers to 2-dimensional distance on the original scanned image, hence glyphs above,\nbelow, to the left, and to the right of the target glyph.) The model parameters are tuned using multilanguage\nsubcorpora, one in each of the 32 supported languages. From the per-glyph probability one can\ncompute an aggregate probability for a sequence of glyphs, including the entire text of a volume. In this\nmanner, every volume has associated with it a probabilistic OCR quality score (quantized to an integer\nbetween 0-100; note that the OCR quality score should not be confused with character or word accuracy).\nIn addition to error detection, the Popat model is also capable of computing the probability that the text is\nin a particular language given any sequence of characters. Thus the algorithm serves the dual purpose of\ndetecting anomalous text while simultaneously identifying the language in which the text is written.\nTo ensure the highest quality data, we excluded volumes with poor OCR quality. For the languages that\nuse a Latin alphabet (English, French, Spanish, and German), the OCR quality is generally higher, and\nmore books are available. As a result, we filtered out all volumes whose quality score was lower than\n80%. For Chinese and Russian, fewer books were available, and we did not apply the OCR filter. For\nHebrew, a 50% threshold was used, because its OCR quality was relatively better than Chinese or\nRussian. For geographically specific corpora, English US and English UK, a less stringent 60% threshold\nwas used, in order to maximize the number of books included (note that, as such, these two corpora are\nnot strict subsets of the broader English corpus). Figure S4 shows the distribution of OCR quality score\nas a function of the fraction of books in the English corpus. Use of an 80% cut off will remove the books\nwith the worst OCR, while retaining the vast majority of the books in the original corpus.\nThe OCR quality scores were also used as a localized indicator of textual quality in order to remove\nanomalous sections of otherwise high-quality texts. The end source text was ensured to be of\ncomparable quality to the post-OCR text presented in \"text-mode\" on the Google Books website.\nII.1C. Accuracy of language metadata\nWe applied additional filters to remove books with dubious language-of-composition metadata. This filter\nremoved volumes whose meta-data language tag disagrees with the language determined by the\nstatistical language detection algorithm described in section 2A. For our English corpus, 8.56%\n6\n(approximately 235,000) of the books were filtered out in this way. Table S1 lists the fraction removed at\nthis stage for our other non-English corpora.\nII.1D. Year Restriction\nIn order to further ensure publication date accuracy and consistency of dates across all our corpora, we\nimplemented a publication year restriction and only retained books with publication years starting from\n1550 and ending in 2008. We found that a significant fraction of mis-dated books have a publication year\nof 0 or dates prior to the invention of printing. The number of books filtered due to this year range\nrestriction is considerably small, usually under 2% of the original number of books.\nThe fraction of the corpus removed by all stages of the filtering is summarized in Table S1. Note that\nbecause the filters are applied in a fixed order, the statistics presented below are influenced by the\nsequence in which the filters were applied. For example, books that trigger both the OCR quality filter and\nby the language correction filter are excluded by the OCR quality filter, which is performed first. Of course,\nthe actual subset of books filtered is the same regardless of the order in which the filters are applied.\nII.2. Metadata based subdivision of the Google Books Collection\nII.2A. Determination of language\nTo create accurate corpora in particular languages that minimize cross-language contamination, it is\nimportant to be able to accurately associate books with the language in which they were written. To\ndetermine the language in which a text is written, we rely on metadata derived from our 100 bibliographic\nsources, as well as statistical language determination using the Popat algorithm (Ref S3). The algorithm\ntakes advantage of the fact that certain character sequences, such as 'the', 'of', and 'ion\", occur more\nfrequently in English. In contrast, the sequences 'la', 'aux', and 'de' occur more frequently in French.\nThese patterns can be used to distinguish between books written in English and those written in\nFrench. More generally, given the entire text of a book, the algorithm can reliably classify the book into\none of the 32 supported language types. The final consensus language was determined based on the\nmetadata sources as well as the results of the statistical language determination algorithm, with the\nstatistical algorithm as the higher priority.\nII.2B. Determination of book subject assignments\nBook subject assignments were determined using a book's Book Industry Standards and Communication\n(BISAC) subject categories. BISAC subject headings are a system for categorizing books based on\ncontent developed by the BISAC subject codes committee overseen by the Book Industry Study Group.\nThey are often used for a variety of purposes, such as to determine how books are shelved in stores. For\nEnglish, 92.4% of the books had at least one BISAC subject assignment. In cases where there were\nmultiple subject assignments, we took the more commonly used subject heading and discarded the rest.\nII.2C. Determination of book country-of-publication\nCountry of publication was determined on the basis of our 100 bibliographic sources; 97% of the books\nhad a country-of-publication assignment. The country code used is the 2 letter code as defined in the ISO\n3166-1 alpha-2 standard. More specifically, when constructing our US versus British English corpora, we\nused the codes \"us\" (United States) and \"gb\" (Great Britain) to filter our volumes.\n7\nII.3. Construction of historical n-grams corpora\nII.3A. Creation of a digital sequence of 1-grams and extraction of n-gram\ncounts\nAll input source texts were first converted into UTF-8 encoding before tokenization. Next, the text of each\nbook was tokenized into a sequence of 1-grams using Google‟s internal tokenization libraries (more\ndetails on this approach can be found in Ref. S4). Tokenization is affected by two processes: (i) the\nreliability of the underlying OCR, especially vis-à-vis the position of blank spaces; (ii) the specific\ntokenizer rules used to convert the post-OCR text into a sequence of 1-grams.\nOrdinarily, the tokenizer separates the character stream into words at the white space characters (\\n\n[newline]; \\t [tab]; \\r [carriage return]; “ “ [space]). There are, however, several exceptional cases:\n(1) Column-formatting in books often forces the hyphenation of words across lines. Thus the word\n“digitized”, may appear on two lines in a book as \"digi-<newline>ized\". Prior to tokenization, we look for 1-\ngrams that end with a hyphen ('-') followed by a newline whitespace character. We then concatenate the\nhyphen-ending 1-gram to the next 1-gram. In this manner, digi-<newline>tized became “digitized”. This\nstep takes place prior to any other steps in the tokenization process.\n(2) Each of the following characters are always treated as separate words:\n! (exclamation-mark)\n@ (at)\n% (percent)\n^ (caret)\n* (star)\n( (open-round-bracket)\n) (close-round-bracket)\n[ (open-square-bracket)\n] (close-square-bracket)\n- (hyphen)\n= (equals)\n{ (open-curly-bracket)\n} (close-curly-bracket)\n| (pipe)\n\\ (backslash)\n: (colon)\n: (semi-colon)\n< (less-than)\n8\n, (comma)\n> (greater-than)\n? (question-mark)\n/ (forward-slash)\n~ (tilde)\n` (back-tick)\n“ (double quote)\n(3) The following characters are not tokenized as separate words:\n& (ampersand)\n_ (underscore)\nExamples of the resulting words include AT&T, R&D, and variable names such as\nHKEY_LOCAL_MACHINE.\n(4) . (period) is treated as a separate word, except when it is part of a number or price, such as 99.99 or\n$999.95. A specific pattern matcher looks for numbers or prices and tokenizes these special strings as\nseparate words.\n(5) $ (dollar-sign) is treated as a separate word, except where it is the first character of a word consisting\nentirely of numbers, possibly containing a decimal point. Examples include $71 and $9.95\n(6) # (hash) is treated as a separate word, except when it is preceded by a-g, j or x. This covers musical\nnotes such as A# (A-sharp), and programming languages j#, and x#.\n(7) + (plus) is treated as a separate word, except it appears at the end of a sequence of alphanumeric\ncharacters or “+” s. Thus the strings C++ and Na2+ would be treated as single words. These cases\ninclude many programming language names and chemical compound names.\n(8) ' (apostrophe/single-quote) is treated as a separate word, except when it precedes the letter s, as in\nALICE'S and Bob's\nThe tokenization process for Chinese was different. For Chinese, an internal CJK\n(Chinese/Japanese/Korean) segmenter was used to break characters into word units. The CJK\nsegmenter inserts spaces along common semantic boundaries. Hence, 1-grams that appear in the\nChinese simplified corpora will sometimes contain strings with 1 or more Chinese characters.\nGiven a sequence of n 1-grams, we denote the corresponding n-gram by concatenating the 1-grams with\na plain space character in between. A few examples of the tokenization and 1-gram construction method\nare provided in Table S2.\nEach book edition was broken down into a series of 1-grams on a page-by-page basis. For each page of\neach book, we counted the number of times each 1-gram appeared. We further counted the number of\ntimes each n-gram appeared (e.g., a sequence of n 1-grams) for all n less than or equal to 5. Because\nthis was done on a page-by-page basis, n-grams that span two consecutive pages were not counted.\n9\nII.3B. Generation of historical n-grams corpora\nTo generate a particular historical n-grams corpus, a subset of book editions is chosen to serve as the\nbase corpus. The chosen editions are divided by publication year. For each publication year, total counts\nfor each n-gram are obtained by summing n-gram counts for each book edition that was published in that\nyear. In particular, three counts are generated: (1) the total number of times the n-gram appears; (2) the\nnumber of pages on which the n-gram appears; and (3) the number of books in which the n-gram\nappears.\nWe then generate tables showing all three counts for each n-gram, resolved by year. In order to ensure\nthat n-grams could not be easily used to identify individual text sources, we did not report counts for any\nn-grams that appeared fewer than 40 times in the corpus. (As a point of reference, the total number of 1-\ngrams that appear in the 3.2 million books written in English with highest date accuracy („eng-all‟, see\nbelow) is 360 billion: a 1-gram that would appear fewer than 40 times occurs at a frequency of the order\nof 10 -11 .) As a result, rare spelling and OCR errors were also omitted. Since most n-grams are infrequent,\nthis also served to dramatically reduce the size of the n-gram tables. Of course, the most robust historical\ntrends are associated with frequent n-grams, so our ability to discern these trends was not compromised\nby this approach.\nBy dividing the reported counts by the corpus size (measured in either words, pages, or books), it is\npossible to determine the normalized frequency with which an n-gram appears in the base corpus. Note\nthat the different counts can be used for different purposes. The usage frequency of an n-gram,\nnormalized by the total number of words, reflects both the number of authors using an n-gram, and how\nfrequently they use it. It can be driven upward markedly by a single author who uses an n-gram very\nfrequently, for instance in a biography of 'Gottlieb Daimler' which mentions his name many times. This\nlatter effect is sometimes undesirable. In such cases, it may be preferable to examine the fraction of\nbooks containing a particular n-gram: texts in different books, which are usually written by different\nauthors, tend to be more independent.\nEleven corpora were generated, based on eleven different subsets of books. Five of these are English\nlanguage corpora, and six are foreign language corpora.\nEng-all\nThis is derived from a base corpus containing all English language books which pass the filters described\nin section 1.\nEng-1M\nThis is derived from a base corpus containing 1 million English language books which passed the filters\ndescribed in section 1. The base corpus is a subset of the Eng-all base corpus.\nThe sampling was constrained in two ways.\nFirst, the texts were re-sampled so as to exhibit a representative subject distribution. Because digitization\ndepends on the availability of the physical books (from libraries or publishers), we reasoned that digitized\nbooks may be a biased subset of books as a whole. We therefore re-sampled books so as to ensure that\nthe diversity of book editions included in the corpus for a given year, as reflected by BISAC subject\ncodes, reflected the diversity of book editions actually published in that year. We estimated the latter\nusing our metadata database, which reflects the aggregate of our 100 bibliographic sources and includes\n10-fold more book editions than the scanned collection.\nSecond, the total number of books drawn from any given year was capped at 6174. This has the net\neffect of ensuring that the total number of books in the corpus is uniform starting around the year 1883.\nThis was done to ensure that all books passing the quality filters were included in earlier years. This\n10\ncapping strategy also minimizes bias towards modern books that might otherwise result because the\nnumber of books being published has soared in recent decades.\nEng-Modern-1M\nThis corpus was generated exactly as Eng-1M above, except that it contains no books from before 1800.\nEng-US\nThis is derived from a base corpus containing all English language books which pass the filters described\nin section 1 but having a quality filtering threshold of 60%, and having 'United States' as its country of\npublication, reflected by the 2-letter country code \"us\",\nEng-UK\nThis is derived from a base corpus containing all English language books which pass the filters described\nin section 1 but having a quality filtering threshold of 60%, and having 'United Kingdom' as its country of\npublication, reflected by the 2-letter country code \"gb\",\nFre-all\nThis is derived from a base corpus containing all French language books which pass the series of filters\ndescribed in section 1.\nGer-all\nThis is derived from a base corpus containing all German language books which pass\nthe series of filters described in section 1.\nSpa-all\nThis is derived from a base corpus containing all Spanish language books which pass the series of filters\ndescribed in section 1.\nRus-all\nThis is derived from a base corpus containing all Russian language books which pass the series of filters\ndescribed in section 1C-D.\nChi-sim-all\nThis is derived from a base corpus containing all books written using the simplified Chinese character set\nwhich pass the series of filters described in section 1C-D.\nHeb-all\nThis is derived from a base corpus containing all Hebrew language books which pass the series of filter\ndescribed in section 1.\n11\nThe computations required to generate these corpora were performed at Google using the MapReduce\nframework for distributed computing (Ref S5). Many computers were used as these computations would\ntake many years on a single ordinary computer.\nNote that the ability to study the frequency of words or phrases in English over time was our primary focus\nin this study. As such, we went to significant lengths to ensure the quality of the general English corpora\nand their date metadata (i.e., Eng-all, Eng-1M, and Eng-Modern-1M). As a result, the accuracy of placeof-publication\ndata in English is not as reliable as the accuracy of date metadata. In addition, the foreign\nlanguage corpora are affected by issues that were improved and largely eliminated in the English data.\nFor instance, their date metadata is not as accurate. In the case of Hebrew, the metadata for language is\nan oversimplification: a significant fraction of the earliest texts annotated as Hebrew are in fact hybrids\nformed from Hebrew and Aramaic, the latter written in Hebrew script.\nThe size of these base corpora is described in Tables S3-S6.\nIII. Culturomic Analyses\nIn this section we describe the computational techniques we use to analyze the historical n-grams\ncorpora.\nIII.0. General Remarks\nIII.0.1 On Corpora.\nThere is significant variation in the quality of the various corpora during various time periods and their\nsuitability for culturomic research. All the corpora are adequate for the uses to which they are put in the\npaper. In particular, the primary object of study in this paper is the English language from 1800-2000; this\ncorpus during this period is therefore the most carefully curated of the datasets. However, to encourage\nfurther research, we are releasing all available datasets - far more data than was used in the paper. We\ntherefore take a moment to describe the factors a culturomic researcher ought to consider before relying\non results of new queries not highlighted in the paper.\n1) Volume of data sampled. Where the number of books used to count n-gram frequencies is too small,\nthe signal to noise ratio declines to the point where reliable trends cannot be discerned. For instance, if\nan n-gram's actual frequency is 1 part in n, the number of words required to create a single reliable\ntimepoint must be some multiple of n. In the English language, for instance, we restrict our study to years\npast 1800, where at least 40 million words are found each year. Thus an n-gram whose frequency is 1\npart per million can be reliably quantified with single-year resolution. In Chinese, there are fewer than 10\nmillion words per year prior to the year 1956. Thus the Chinese corpus in 1956 is not in general as\nsuitable for reliable quantification as the English corpus in 1800. (In some cases, reducing the resolution\nby binning in larger windows can be used to sample lower frequency n-grams in a corpus that is too smal\nfor single-year resolution.) In sum: for any corpus and any n-gram in any year, one must consider whether\nthe size of the corpus is sufficient to enable reliable quantitation of that n-gram in that year.\n2) Composition of the corpus. The full dataset contains about 4% of all books ever published, which\nlimits the extent to which it may be biased relative to the ensemble of all surviving books. Still, marked\nshifts in composition from one year to another are a potential source of error. For instance, book sampling\npatterns differ for the period before the creation of Google Books (2004) as compared to the period\nafterward. Thus, it is difficult to compare results from after 2000 with results from before 2000. As a result,\nsignificant changes in culturomic trends past the year 2000 may reflect corpus composition issues. This\nwas an important reason for our choice of the period between 1800 and 2000 as the target period.\n12\n3) Quality of OCR. This varies from corpus to corpus as described above. For English, we spent a great\ndeal of time examining the data by hand as an additional check on its reliability. The other corpora may\nnot be as reliable.\n4) Quality of Metadata. Again, the English language corpus was checked very carefully and\nsystematically on multiple occasions, as described above and in the following sections. The metadata for\nthe other corpora may not be equally reliable for all periods. In particular, the Hebrew corpus during the\n19th century is composed largely of reprinted works, whose original publication dates farpredate the\nmetadata date for the publication of the particular edition in question. This must be borne in mind for\nresearchers intent on working with that corpus.\nIn addition to these four general issues, we note that earlier portions of the Hebrew corpus contain a large\nquantity of Aramaic text written in Hebrew script. As these texts often oscillate back and forth between\nHebrew and Aramaic, they are particularly hard to accurately classify.\nAll the above issues will likely improve in the years to come. In the meanwhile, users must use extra\ncaution in interpreting the results of culturomic analyses, especially those based on the various non-\nEnglish corpora. Nevertheless, as illustrated in the main text, these corpora already contain a great\ntreasury of useful material, and we have therefore made them available to the scientific community\nwithout delay. We have no doubt that they will enable many more fascinating discoveries.\nIII.0.2 On the number of books published\nIn the text, we report that our corpus contains about 4% of all books ever published. Obtaining this\nestimate relies on knowing how many books are in the corpus (5,195,769) and estimating the total\nnumber of books ever published. The latter quantity is extremely difficult to estimate, because the record\nof published books is fragmentary and incomplete, and because the definition of book is itself\nambiguous.\nOne way of estimating the number of books ever published is to calculate the number of editions in the\ncomprehensive catalog of books which was described in Section I of the supplemental materials. This\nproduces an estimate of 129 million book editions. However, this estimate must be regarded with great\ncaution: it is conservative, and the choice of parameters for the clustering algorithm can lead to significant\nvariation in the results. More details are provided in Ref S1.\nAnother independent estimate we obtained in the study \"How Much Information? (2003)\" conducted at\nBerkeley (Ref S6). That study also produced a very rough estimate of the number of books ever\npublished and concluded that it was between 74 million and 175 million.\nThe results of both estimates are in general agreement. If the actual number is closer to the low end of\nthe Berkeley range, then our 5 million book corpus encompasses a little more than 5% of all books ever\npublished; if it is at the high end, then our corpus would constitute a little less than 3%. We report an\napproximate value (about 4%) in the text; it is clear that, in the coming years, more precise estimates of\nthe denominator will become available.\nIII.1. Generation of timeline plots\nIII.1A. Single Query\nThe timeline plots shown in the paper are created by taking the number of appearances of an n-gram in a\ngiven year in the specified corpus and dividing by the total number of words in the corpus in that year.\nThis yields a raw frequency value. Results are smoothed using a three year window; i.e., the frequency of\n13\na particular n-gram in year X as shown in the plots is the mean of the raw frequency value for the n-gram\nin the year X, the year X-1, and the year X+1.\nNote that for each n-gram in the corpus, we can provide three measures as a function of year of\npublication:\n1- the number of times it appeared\n2- the number of pages where it appeared\n3- the number of books where it appeared.\nThroughout the paper, we make use only of the first measure; but the two others remain available. They\nare generally all in agreement, but can denote distinct cultural effects. These distinctions are not explored\nin this paper.\nFor example, we give in Appendix measures for the frequency of the word 'evolution'. In the first three\ncolumns, we give the number of times it appeared, the normalized number of times it appeared (relative\nto #words that year), the normalized number of pages it appeared in, and the normalized number of\nbooks it appeared in, as a function of the date.\nIII.1B. Multiple Query/Cohort Timelines\nWhere indicated, timeline plots may reflect the aggregates of multiple query results, such as a cohort of\nindividuals or inventions. In these cases, the raw data for each query we used to associate each year with\na set of frequencies. The plot was generated by choosing a measure of central tendency to characterize\nthe set of frequencies (either mean or median) and associating the resulting value with the corresponding\nyear.\nSuch methods can be confounded by the vast frequency differences among the various constituent\nqueries. For instance, the mean will tend to be dominated by the most frequent queries, which might be\nseveral orders of magnitude more frequent than the least frequent queries. If the absolute frequency of\nthe various query results is not of interest, but only their relative change over time, then individual query\nresults may be normalized so that they yield a total of 1. This results in a probability mass function for\neach query describing the likelihood that a random instance of a query derives from a particular year.\nThese probability mass functions may then be summed to characterize a set of multiple queries. This\napproach eliminates bias due to inter-query differences in frequency, making the change over time in the\ncohort easier to track.\nIII.2. Note on collection of historical and cultural data\nIn performing the analyses described in this paper, we frequently required additional curated datasets of\nvarious cultural facts, such as dates of rule of various monarchs, lists of notable people and inventions,\nand many others. We often used Wikipedia in the process of obtaining these lists. Where Wikipedia is\nmerely digitizing the content available in another source (for instance, the blacklists of Wolfgang\nHermann), we corrected the data using the original sources. In other cases this was not possible, but we\nfelt that the use of Wikipedia was justifiable given that (i) the data – including all prior versions - is publicly\navailable; (ii) it was created by third parties with no knowledge of our intended analyses; and (iii) the\nspecific statistical analyses performed using the data were robust to errors; i.e., they would be valid as\nlong as most of the information was accurate, even if some fraction of the underlying information was\nwrong. (For instance, the aggregate analysis of treaty dates as compared to the timeline of the\ncorresponding treaty, shown in the control section, will work as long as most of the treaty names and\ndates are accurate, even if some fraction of the records is erroneous.\nWe also used several datasets from the Encyclopedia Britannica, to confirm that our results were\nunchanged when high-quality carefully curated data was used. For the lexicographic analyses, we relied\nprimarily on existing data from the American Heritage Dictionary.\nWe avoided doing manual annotation ourselves wherever possible, in an effort to avoid biasing the\nresults. When manual annotation had to be performed, such as in the classification of samples from our\n14\nlanguage lexica, we tried whenever possible to have the annotation performed by a third party with no\nknowledge of the analyses we were undertaking\nIII.3. Controls\nTo confirm the quality of our data in the English language, we sought positive controls in the form of\nwords that should exhibit very strong peaks around a date of interest. We used three categories of such\nwords: heads of state („President Truman‟), treaties („Treaty of Versailles‟), and geographical name\nchange („Byelorussia‟ to „Belarus‟). We used Wikipedia as a primary source of such words, and manually\ncurated the lists as described below. We computed the timeserie of each n-gram, centered it on the date\nof interest (year when the person became president, for instance), and normalized the timeserie by\noverall frequency. Then, we took the mean trajectory for each of the three cohorts, and plotted in Figure\nS5.\nThe list of heads of states include all US presidents and British monarchs who gained power in the 19 th or\n20 th centuries (we removed ambiguous names, such as „President Roosevelt‟). The list of treaties is taken\nfrom the list of 198 treaties signed in the 19 th or 20 th centuries (S7); but we kept only the 121 names that\nreferred to only one known treaty, and that have non zero timeseries. The list of country name changes is\ntaken from Ref S8. The lists are given in APPENDIX.\nThe correspondence between the expected and observed presence of peaks was excellent. 42 out of 44\nheads of state had a frequency increase of over 10-fold in the decade after they took office (expected if\nthe year of interest was random: 1). Similarly, 85 out of 92 treaties had a frequency increase of over 10-\nfold in the decade after they were signed (expected: 2). Last, 23 out of 28 new country names became\nmore frequent than the country name they replaced within 3 years of the name change; exceptions\ninclude Kampuchea/Cambodia (the name Cambodia was later reinstated), Iran/Persia (Iran is still today\nreferred to as Persia in many contexts) and Sri Lanka/Ceylon (Ceylon is also a popular tea).\nIII.4. Lexicon Analysis\nIII.4A. Estimation of the number of 1-grams defined in leading\ndictionaries of the English language.\n(a) American Heritage Dictionary of the English Language, 4th Edition (2000)\nWe are indebted to the editorial staff of AHD4 for providing us the list of the 153,459 headwords that\nmake up the entries of AHD4. However, many headwords are not single words (“preferential voting” or\n“men‟s room”), and others are listed as many times as there are grammatical categories (“to console”, the\nverb; “console”, the piece of furniture).\nAmong those entries, we find 116,156 unique 1-grams (such as “materialism” or “extravagate”).\n15\n(b) Webster’s Third New International Dictionary (2002)\nThe editorial staff communicated to us the number of “boldface entries” of the dictionary, which are taken\nto be the number of n-grams defined: 476,330.\nThe editorial staff also communicated the number of multi-word entries 74,000 out of a total number of\nentries 275,000. They estimate a lower bound of multi-word entries at 27% of the entries.\nTherefore, we estimate an upper bound of unique 1-grams defined by this dictionary as 0.27*476,330,\nwhich is approximately 348,000.\n(c) Oxford English Dictionary (Reference in main text)\nFrom the website of the OED we can read that the “number of word forms defined and/or illustrated” is\n615,100; and that we find 169,000 “italicized-bold phrases and combinations”.\nTherefore, we estimate an upper bound of the number of unique 1-grams defined by this dictionary as\n615,100-169,000 which is approximately 446,000.\nIII.4B. Estimation of Lexicon Size\nHow frequent does a 1-gram have to be in order to be considered a word? We chose a minimum\nfrequency threshold for „common‟ 1-grams by attempting to identify the largest frequency decile that\nremains lower than the frequency of most dictionary words.\nWe plotted a histogram showing the frequency of the 1-grams defined in AHD4, as measured in our year\n2000 lexicon. We found that 90% of 1-gram headwords had a frequency greater than 10 -9 , but only 70%\nwere more frequent than 10 -8 . Therefore, the frequency 10 -9 is a reasonable threshold for inclusion in the\nlexicon.\nTo estimate the number of words, we began by generating the list of common 1-grams with a higher\nchronological resolution, namely 11 different time points from 1900 until 2000 (1900, 1910, 1920, ... 2000)\nas described above. We next excluded all 1-grams with non-alphabetical characters in order to produce a\nlist of common alphabetical forms for each time point.\nFor three of the time points (1900, 1950, 2000), we took a random sample of 1000 alphabetical forms\nfrom the resulting set of alphabetical forms. These were classified by a native English speaker with no\nknowledge of the analyses being performed. The results of the classification are found in Appendix. We\nasked the speaker to classify the candidate words were classified into 8 categories:\nM if the word is a misspelling or a typo or seems like gibberish*\nN if the word derives primarily from a personal or a company name\nP for any other kind of proper nouns\nH if the word has lost its original hyphen\nF if the word is a foreign word not generally used in English sentences\nB if it is a „borrowed‟ foreign word that is often used in English sentences\nR for anything that does not fall into the above categories\nU unclassifiable for some reason\nWe computed the fraction of these 1000 words at each time point that were classified as P, N, B, or R,\nwhich we call the „word fraction for year X‟, or WF X . To compute the estimated lexicon size for 1900,\n1950, and 2000, we multiplied the word fraction by the number of alphabetical forms in those years.\nFor the other 8 time points, we did not perform a separate sampling step. Instead, we estimated the word\nfraction by linearly interpolating the word fraction of the nearest sampled time points; i.e., the word\nfraction in 1920 satisfied WF 1920 =.WF 1900 +.4*(WF 1950 .- WF 1900 ). We then multiplied the word fraction by the\nnumber of alphabetical forms in the corresponding year, as above.\nFor the year 2000 lexicon, we repeated the sampling and annotation process using a different native\nspeaker. The results were similar, which confirmed that our findings were independent of the person\ndoing the annotation.\nWe note that the trends shown in Fig 2A are similar when proper nouns (N) are excluded from the lexicon\n(i.e., the only categories are P, B and R). Figure S7 shows the estimates of the lexicon excluding the\ncategory „N‟ (proper nouns).\n* A typo is a one-time typing error by someone who presumably knows the correct spelling (as in\nimprotant); a misspelling, which generally has the same pronunciation as the correct spelling, arises when\na person is ignorant of the correct spelling (as in abberation).\n16\nIII.4C. Dictionary Coverage\nTo determine the coverage of the OED and Merriam-Webster‟s Unabridge Dictionary (MW), we\nperformed the above analysis on randomly generated subsets of the lexicon in eight frequency deciles\n(ranging from 10 -9 – 10 -8 to 10 -3 – 10 -2 ). The samples contained 500 candidate words each for all but the\ntop 3 deciles; the samples corresponding to the top 3 deciles (10 -5 – 10 -4 , 10 -4 – 10 -3 , 10 -3 – 10 -2 )\ncontained 100 candidate words each.\nA native speaker with no knowledge of the experiment being performed determined which words from our\nrandom samples fell into the P, B, or R categories (to enable a fair comparison, we excluded the N\ncategory from our analysis as both OED an MW exclude them). The annotator then attempted to find a\ndefinition for the words in both the online edition of the Merriam-Webster Unabridged Dictionary or in the\nonline version of the Oxford English Dictionary‟s 2 nd edition. Notably, the performance of the latter was\nboosted appreciably by its inclusion of Merriam-Webster‟s Medical Dictionary. Results of this analysis are\nshown in Appendix.\nTo estimate the fraction of dark matter in the English language, we applied the formula:\nsum over all deciles of P word *P OED/MW *N 1gram , with:\n- N 1gram the number of 1grams in the decile\n- P word the proportion of words (R,B or P) in this decile\n- P OED/MW the proportion of words of that decile that are covered in OED or MW.\nWe obtain 52% of dark matter, words not listed in either MW or the OED. With the procedure above, we\nestimate the number of words excluding proper nouns at 572,000; this results in 297,000 words unlisted\nin even the most comprehensive commercial and historical dictionaries.\nIII.4D. Analysis New and Obsolete words in the American Heritage\nDictionary\nWe obtained a list of the 4804 vocabulary items that were added to the AHD4 in 2000 from the\ndictionary‟s editorial staff. These 4804 words were not in AHD3 (1992) – although, on rare occasions a\nword could have featured in earlier editions of the dictionary (this is the case for “gypseous”, which was\nincluded in AHD1 and AHD2).\nSimilar to our study of the dictionary‟s lexicon, we restrict ourselves to 1grams. We find 2077 1-grams\nnewly added to the AHD4. Median frequency (Fig 2D) is computed by obtaining all frequencies of this set\nof words and computing its median.\nNext, we ask which 1grams appear in AHD4 but are not part of the year 2000 lexicon any more\n(frequency lower than one part per billion between 1990 and 2000). We compute the lexical frequency of\nthe 1-gram headwords in AHD, and find a small number (2,220) that are not part of the lexicon today. We\nshow the mean frequency of these 2,220 words (Fig 2F).\nIII.5. The Evolution of Grammar\nIII.5A. Ensemble of verbs studied\nOur list of irregular verbs was derived from the supplemental materials of Ref 18 (main text). The full list\nof 281 verbs is given in Appendix.\nOur objective is to study the way word frequency affects the trajectories of the irregular compared with\nregular past tense. To do so, we must be confident that\n- the 1grams used refer to the verbs themselves: “to dive/dove” cannot be used, as “dove” is a\ncommon noun for a bird. Or, in the verb “to bet/bet”, the irregular preterit cannot be distinguished from the\n17\npresent (or, for that matter, from the common noun “a bet”).\n- the verb is not a compound, like “overpay” or “unbind”, as the effect of the underlying verb\n(“pay”, “bind”) is presumably stronger than that of usage frequency.\nWe therefore obtain a list of 106 verbs that we use in the study (marked by the denomination „True‟ in the\ncolumn “Use in the study?”)\nIII.5B. Verb frequencies\nNext, for each verb, we computed the frequency of the regular past tense (built by suffixation of „-ed‟ at\nthe end of the verb), and the frequency of the irregular past tense (summing preterit and past participle).\nThese trajectories are represented in Fig 3A and Fig S8.\nWe define the regularity of a verb: at any given point in time, the regularity of a verb is the percentage of\npast tense usage made using the regular version. Therefore, in a given year, the regularity of a verb is\nr=R/(R+I) where R is the number of times the regular past tense was used, and I the number of times the\nirregular past tense was used. The regularity is a continuous variable that ranges between 0 and 1\n(100%).\nWe plot in Figure 3B the mean regularity between 1800-1825 in x-axis, and the mean regularity between\n1975-2000 in y-axis.\nIf we assume that a speaker of the English language uses only one of the two variants (regular or\nirregular); and that all speakers of English are equally likely to use the verb; then the regularity translates\ndirectly into percentage of the population of speakers using the regular form. While these assumptions\nmay not hold generally, they provide a convenient way of estimating the prevalence of a certain word in\nthe population of English speakers (or writers).\nIII.5C. Rates of regularization\nWe can compute, for any verb, the slope of regularity as a function of time: this can be interpreted as the\nvariation in percentage of the population of English speakers using the regular form.\nBy holding population size constant over the time window used to obtain the slope, we derive the\nvariation of population using the regular form in absolute terms.\nFor instance, the regularity of “sneak/snuck” has decreased from 100% to 50% over the past 50 years,\nwhich is 1% per year. We consider the population of US English speakers to be roughly 300 million. As a\nresult, snuck is sneaking in at a speed of 3 million speakers per year, or about one speaker per minute in\nthe US.\nIII.5D. Classification of Verbs\nThe verbs were classified into different types based on the phonetic pattern they represented using the\nclassification of Ref 18 (main text). Fig 3C shows the median regularity for the verbs „burn‟, „spoil‟, „dwell‟,\n„learn‟, „smell‟, „spill‟ in each year. We compute the UK rate as above, using 60 million for UK population.\nIII.6. Collective Memory\nOne hundred timelines were generated, for every year between 1875 and 1975. Amplitude for each plot\nwas measured by either computing „peak height‟ – i.e., the maximum of all the plotted values, or „areaunder-the\ncurve‟ – i.e., the sum of all the plotted values. The peak for year X always occurred within a\n18\nhandful of years after the year X itself. The lag between a year and its peak is partly due to the length of\nthe authorship and publication process. For instance, a book about the events of 1950 may be written\nover the period from 1950-1952 and only published in 1953.\nFor each year, we estimated the slope of the exponential decay shortly past its peak. The exponent was\nestimated using the slope of the curve on a logarithmic plot of frequency between the year Y+5 and the\nyear Y+25. This estimate is robust to the specific values of the interval, as long as the first value (here,\nY+5) is past the peak of Y, and the second value is in the fifty years that follow Y. The Inset in Figure 4A\nwas generated using 5 and 25. The half-life could thus be derived.\nHalf-life can also be estimated directly by asking how many years past the peak elapse before frequency\ndrops below half its peak value. These values are noisier, but exhibit the same trend as in Figure 4A,\nInset (not shown).\nTrends similar to those described here may capture more general events, such as those shown in Figure\nS9.\nIII.7. The Pursuit of Fame\nWe study the fame of individuals appearing in the biographical sections of Encyclopedia Britannica and\nWikipedia. Given the encyclopedic objective of these sources, we argue these represent comprehensive\nlists of notable individuals. Thus, from Encyclopedia Britannica and Wikipedia, we produce databases of\nall individuals born between 1800-1980, recording their full name and year of birth. We develop a method\nto identify the most common, relevant names used to refer to all individuals in our databases. This\nmethod enables us to deal with potentially complicated full names, sometimes including multiple titles and\nmiddle names. On the basis of the amount of biographical information regarding each individual, we\nresolve the ambiguity arising when multiple individuals share some part, or all, their name. Finally, using\nthe time series of the word frequency of people‟s name, we compare the fame of individuals born in the\nsame year or having the same occupation.\nIII.7A) Complete procedure\n7.A.1 - Extraction of individuals appearing in Wikipedia.\nWikipedia is a large encyclopedic information source, with an important number of articles referring to\npeople. We identify biographical Wikipedia articles through the DBPedia engine (Ref S9), a relational\ndatabase created by extensively parsing Wikipedia. For our purposes, the most relevant component of\nDBPedia is the “Categories” relational database.\nWikipedia categories are structural entities which unite articles related to a specific topic. The DBPedia\n“Categories” database includes, for all articles within Wikipedia, a complete listing of the categories of\nwhich this article is a member. As an example, the article for Albert Einstein\n(http://en.wikipedia.org/wiki/Albert_Einstein) is a member of 73 categories, including “German physicists”,\n“American physicists”, “Violonists”, “People from Ulm” and “1879_births”. Likewise, the article for Joseph\nHeller (http://en.wikipedia.org/wiki/Joseph_Heller) is a member of 23 categories, including “Russian-\nAmerican Jews”, “American novelists”, “Catch-22” and “1923_births”.\nWe recognize articles referring to non-fictional people by their membership in a “year_births” category.\nThe category “1879_births” includes Albert Einstein, Wallace Stevens and Leon Trotsky ,likewise\n“1923_births” includes Henry Kissinger, Maria Callas and Joseph Heller while “1931_births” includes\nMichael Gorbachev, Raul Castro and Rupert Murdoch. If only the approximate birth year of a person is\n19\nknown, their article will be a member of a “decade_births” category such as “1890s_births” and\n“1930s_births”. We treat these individuals as if born at the beginning of the decade.\nFor every parsed article, we append metadata relating to the importance of the article within Wikipedia,\nnamely the size in words of the article and the number of page views which it obtains. The article word\ncount is created by directly accessing the article using its URL. The traffic statistics for Wikipedia articles\nare obtained from http://stats.grok.se/.\nFigure S10a displays the number of records parsed from Wikipedia and retained for the final cohort\nanalysis. Table S7 displays specific examples from the extraction‟s output, including name, year of birth,\nyear of death, approximate word count of main article and traffic statistics for March 2010.\n1) Create a database of records referring to people born 1800-1980 in Wikipedia.\na. Using the DBPedia framework, find all articles which are members of the categories\n„1700_births‟ through „1980_births‟. Only people both in 1800-1980 are used for the\npurposes of fame analysis. People born in 1700-1799 are used to identify naming\nambiguities as described in section III.7.A.7 of this Supplementary Material.\nb. For all these articles, create a record identified by the article URL, and append the birth\nyear.\nc. For every record, use the URL to navigate to the online Wikipedia page. Within the main\narticle body text, remove all HTML markup tags and perform a word count. Append this\nword count to the record.\nd. For every record, use the URL to determine the page‟s traffic statistics for the month of\nMarch 2010. Append the number of views to the record.\nIII.7.A.2 – Identification of occupation for individuals appearing in Wikipedia.\nTwo types of structural elements within Wikipedia enable us to identify, for certain individuals, their\noccupation. The first, Wikipedia Categories, was previously described and used to recognize articles\nabout people. Wikipedia Categories also contain information pertaining to occupation. The categories\n“Physicists”, “Physicists by Nationality”, “Physicists stubs”, along with their subcategories, pinpoint articles\nof relating to the occupation of physicist. The second are Wikipedia Lists, special pages dedicated to\nlisting Wikipedia articles which fit a precise subject. For physicists, relevant examples are “List of\nphysicists”, “List of plasma physicists” and “List of theoretical physicists”. Given their redundancy, these\ntwo structural elements, when used in combination provide a strong means of identifying the occupation\nof an individual.\nNext, we selected the top 50 individuals in each category, and annotated each one manually as a function\nof the individual‟s main occupation, as determined by reading the associated Wikipedia article. For\ninstance, “Che Guevara” was listed in Biologists; so even though he was a medical doctor by training, this\nis not his primary historical contribution. The most famous individuals of each category born between\n1800 and 1920 are given in Appendix.\nIn our database of individuals, we append, when available, information about the occupations of people.\nThis enables the comparison, on the basis of fame, of groups of individuals distinguished by their\noccupational decisions.\n20\n2) Associate Wikipedia records of individuals with occupations using relevant Wikipedia\n“Categories” and “Lists” pages. For every occupation to be investigated :\na. Manually create a list of Wikipedia categories and lists associated with this defined\noccupation.\nb. Using the DBPedia framework, find all the Wikipedia articles which are members of the\nchosen Wikipedia categories.\nc. Using the online Wikipedia website, find all Wikipedia articles which are listed in the body\nof the chosen Wikipedia lists.\nd. Intersect the set of all articles belonging to the relevant Lists and Categories with the set\nof people both 1800-1980. For people in both sets, append the occupation information.\ne. Associate the records of these articles with the occupation.\nIII.7.A.3 - Extraction of individuals appearing in Encyclopedia Britannica.\nEncyclopedia Britannica is a hand-curated, high quality encyclopedic dataset with many detailed\nbiographical entries. We obtained, in a private communication, structured datasets from Encyclopedia\nBritannica Inc. These datasets contain a complete record of all entries relating to individuals in the\nEncyclopedia Britannica. Each record contains the birth and death of the person at hand, as well as set of\ninformation snippets summarizing the most critical biographical information available within the\nencyclopedia.\nFor the analysis of fame, we extract, from the dataset provided by Encyclopedia Britannica Inc.,\nrecords of individuals born in between 1800 and 1980. For every person, we retain, as a measure of their\nnotability, a count of the number of biographical snippets present in the dataset. Figure S10b outlines the\nnumber of records parsed from the Encyclopedia Britannica dataset, as well as the number of these\nrecords ultimately retained for final analysis. Table S8 displays examples of records parsed in this step of\nthe analysis procedure.\n3) Create a database of records referring to people born 1800-1980 in Encyclopedia\nBritannica.\na. Using the internal database records provided by Encyclopedia Britannica Inc., find all\nentries referring to individuals born 1700-1980. Only people both in 1800-1980 are used\nfor the purposes of fame analysis. People born in 1700-1799 are used to identify naming\nambiguities as described in section III.7.A.7 of this Supplementary Material.\nb. For these entries, create a record identified by a unique integer containing the individual‟s\nfull name, as listed in the encyclopedia, and the individual‟s birth year.\nc. For every record, find the number of encyclopedic informational snippets present in the\nEncyclopedia Britannica dataset. Append this count to the record.\nIII.7.A.4 – Produce spelling variants of the full names of individuals.\nWe ultimately wish to identify the most relevant name used to commonly refer to an individual. Given the\nlimits of OCR and the specificities of the method used to create the word frequency database, certain\ntypographic elements such as accents, hyphens or quotation marks can complicate this process. As\nsuch, for every full name present in our database of people, we append variants of the full names where\nthese typographic elements have been removed or, when possible, replaced. Table S9 presents\nexamples of spelling variants for multiple names.\n21\n4) In both databases, for every record, create a set of raw names variants. To create the set:\na. Include the original raw name.\nb. If the name includes apostrophes or quotation marks, include a variant where these\nelements are removed.\nc. If the first word in the name contains a hyphen, include a name where this hyphen is\nreplaced with a whitespace.\nd. If the last word of the name is a numeral, include a name where this numeral has been\nremoved.\ne. For every element in the set which contains non-Latin characters, include a variant where\nthis characters have been replaced using the closest Latin equivalent.\nIII.7.A.5 – Find possible names used to refer to individuals.\nThe common name of an individual sometimes significantly differs from the complete, formal name\npresent in Encyclopedia Britannica and Wikipedia. This encyclopedia full name can contain details such\nas titles, initials and military or nobility standings, which are not commonly used when referring to\nindividual in most publications. Even in simpler cases, when the full name contains only first, middle and\nlast names, there exists no systematic convention on which names to use when talking about an\nindividual. Henry David Thoreau is most commonly referred to by his full name, not “Henry Thoreau” nor\n“David Thoreau”, whereas Oliver Joseph Lodge is mentioned by his first and last name “Oliver Lodge”,\nnot his full name “Oliver Joseph Lodge”.\nGiven a full name with complex structure potentially containing details such as titles, initials, nobility rights\nand ranks, in addition to multiple first and last names, we must extract a list of simple names, using three\nwords at most, which can potentially be used to refer to this individual. This set of names is created by\ngenerating combinations of names found in the raw name. Furthermore, whenever they appear we\nsystematically exclude common words such as titles or ranks from these names. The query name sets of\nseveral individuals are displayed in Table S10.\n5) For every record, using the set of raw names, create a set of query names. Query names\nare (2,3) grams which will be used in order to measure the fame of the individual. The following\nprocedure is iterated on every raw name variant associated with the record. Steps for which the\nrecord type is not specified are carried out for both.\na. For Encyclopedia Britannica records, truncate the raw name at the second comma,\nreorder so that the part of name preceding the first comma follows that succeeding the\ncomma.\nb. For Wikipedia records, replace the underscores with whitespaces.\nc. Truncate the name string at the first (if any) parenthesis or comma.\nd. Truncate the name string at the beginning of the words „in‟, ‟In‟, ‟the‟, ‟The‟, ‟of‟ and „Of‟, if\nthese are present.\ne. Create the last name set. Iterating from last to first in the words of the name, add the first\nname with the following properties:\ni. Begin with a capitalized letter.\nii. Longer than 1 character.\niii. Not ending in a period.\niv. If the words preceding this last name are identified as a prefix ('von', 'de', 'van',\n'der', 'de' , „d'‟, 'al-', 'la', 'da', 'the', 'le', 'du', 'bin', 'y', 'ibn' and their capitalized\nversions ), the last name is a 2gram containing both the prefix.\nf. If the last name contains a capitalized character besides the first one, add a variant of\nthis word where the only capital letter is the first to the set of last names.\ng. Create the set of first names. Iterating on the raw name elements which are not part of\nthe last name set, candidate first names are words with the following properties :\ni. Begin with a capital letter.\nii. Longer than 1 character.\niii. Not ending in a period.\niv. Not a title. („Archduke‟, 'Saint', 'Emperor', 'Empress', 'Mademoiselle', 'Mother',\n'Brother', 'Sister', 'Father', 'Mr', 'Mrs', 'Marshall', 'Justice', 'Cardinal', 'Archbishop',\n'Senator', 'President', 'Colonel', 'General', 'Admiral', 'Sir', 'Lady', 'Prince',\n'Princess', 'King', 'Queen', 'de', 'Baron', 'Baroness', 'Grand', 'Duchess', 'Duke',\n'Lord', 'Count', 'Countess', 'Dr')\n22\nh. Add to the set of query names all pairs of “first names + last names” produced by\ncombining the sets of first and last names.\ni. This procedure is carried for every raw name variant.\nIII.7.A.6 – Find the word match frequencies of all names.\nGiven the set of names which may refer to an individual, we wish to find the time resolved words\nfrequencies of these names. The frequency of the name, which corresponds to a measure of how often\nan individual is mentioned, provides a metric for the fame of that person. We append the word\nfrequencies of all the names which can potentially refer to an individual. This enables us, in a later step,\nto identify which name is the relevant.\n6) Append the fame signal for each query name of each record. The fame signal is the\ntimeseries of normalized word matches in the complete English database.\nIII.7.A.7 – Find ambiguous names which can refer to multiple individuals.\nCertain names are particularly popular and are shared by multiple people. This results in ambiguity, as\nthe same query name may refer to a plurality of individuals. Homonimity conflicts occur between a group\nof individuals when they share some part of, or all, their name. When these homonimity conflicts arise,\nthe word frequency of a specific name may not reflect the number of references to a unique person, but to\nthat of an entire group. As such, the word frequency does not constitute a clear means of tracking the\nfame of the concerned individuals. We identify homonimity conflicts by finding instances of individuals\nwhose names contain complete or partial matches. These conflicts are, when possible, resolved on the\nbasis of the importance of the conflicted individuals in the following step. Typical homonimity conflicts are\nshown in Table S11.\n7) Identify homonimity conflicts. Homonimity conflicts arise when the query names of two or more\nindividuals contain a substring match. These conflicts are distinguished as such :\na. For every query name of every record, find the set of substrings of query names.\nb. For every query name of every record, search for matches in the set of query name\nsubstrings of all other records.\nc. Bidirectional homonimity conflicts occur when a query name fully matches another query\nname. The name conflicted name could be used to refer to both individuals.\nUnidirectional conflicts occur when a query name has a substring match within another\nquery name. Thus, the conflicted name can refer to one of the individuals, but also be\npart of a name referring to another.\nIII.7.A.8 – Resolve, when possible, the most likely origin of ambiguous names.\nThe problem of homonymous individuals is limiting because the word frequencies data do not allow us to\nresolve the true identity behind a homonymous name. Nonetheless, in some cases, it is possible to\ndistinguish conflicted individuals on the basis of their importance. For the database of people extracted\nfrom Encyclopedia Britannica, we argue that the quantity of information available about an individual\nprovides a proxy for their relevance. Likewise, for people obtained from Wikipedia, we can judge their\nimportance by the size of the article written about the person and the quantity of traffic the article\ngenerates. As such, we approach the problem of ambiguous names by comparing the notability of\nindividuals, as evaluated by the amount of information available about them in the respective\nencyclopedic source. Examples of conflict resolution are shown in Table S12 and S13.\n8) Resolve homonimity conflicts.\n23\na. Conflict resolution involves the decision of whether a query name, associated with\nmultiple records, can unambiguously refer to a single one of them.\nb. Wikipedia. Conflict resolution for Wikipedia records is carried out on the basis the main\narticle word count and traffic statistics. A conflict is resolved as such :\ni. Find the cumulative word count of words written in the articles in conflict.\nii. Find the cumulative number of views resulting from the traffic to the articles in\nconflict.\niii. For every record in the conflict, find the fraction of words and views resulting from\nthis record by dividing by the cumulative counts.\niv. Does a record have the largest fraction of both words written and page views?\nv. Does this record have above 66% of either words written and page views?\nvi. If so, the conflicted query name can be considered as being sufficiently specific\nto the record with these properties.\nc. Encyclopedia Britannica. Conflict resolution for Encyclopedia Britannica records is carried\non the basis of the quantity of information snippets present in the dataset.\ni. Find the cumulative number of information snippets related to the records in\nconflicts.\nii. For every record in the conflict, find the fraction of informational snippets by\ndividing with the cumulative count\niii. If a record has greater than 66% of the cumulative total, the query name in\nconflict is considered to refer to this record.\nIII.7.A.9 Identify the most relevant name used to refer to an individual.\nSo far, we have obtained, for all individuals in both our databases, a set of names by which they can\nplausibly be mentioned. From this set, we wish to identify the best such candidate and use its word\nfrequency to observe the fame of the person at hand. This optimal name is identified on the basis of the\namplitude of the word frequency, the potential ambiguities which arise from name homonimity and the\nquality of the word frequency time series. Examples are shown in Fig S11 and S12.\n9) Determine the best query name for every record.\na. Order all the query names associated with a record on the basis of the integral of the\nfame signal from the year of birth until the year 2000.\nb. Iterating from the strongest fame signal to the lowest, the selected query name is the first\nresult with the following properties :\ni. Unambiguously refers to the record (as determined by conflict resolution, if\nneeded).\nii. The average fame signal in the window [year of birth ± 10 years] is less than 10 -9\nor an order of magnitude less than the average fame signal from the year of birth\nto the year 2000.\niii. (Wikipedia Only). The query name, when converted to a Wikipedia URL by\nreplacing whitespaces with underscores, refers to the record or an inexistent\narticle. If the name refers to another article or a disambiguation page, the query\nname is rejected.\nc. If the best query name is a 2-gram name corresponding the last two names in 3-gram\nquery name, and if the fame integral of the 3-gram name is 80% of the fame integral of\nthe 2-gram, the best query name is replaced by the 3-gram.\n24\nIII.7.A.10 – Compare the fame of multiple individuals.\nHaving identified the best name candidate for every individual, we use the word frequency time series of\nthis name as a metric for the fame of the each individual. We now compare the fame of multiple\nindividuals on the basis of the properties of their fame signal. For this analysis, we group people\naccording to specific characteristics, which in the context of this work are the years of birth and the\nrespective occupations.\n10) Assemble cohorts on the basis of a shared record property.\na. Fetch all records which match a specific record property, such as year of birth or\noccupation.\nb. Create fame cohorts comparing the fame of individuals born in the same year.\ni. Use average lifetime fame ranking, done on the basis of the average fame as\ncomputed from the birth of the individual to the year 2000.\nc. Create fame cohorts for individuals with the same occupation.\ni. Use most famous 20 th year, ranking on the basis of the 20 th best year in the\nterms of fame for the individual.\nIII.7B. Cohorts of fame\nFor each year, we defined a cohort of the top 50 most famous individuals born that year. Individual fame\nwas measured in this case by the average frequency over all years after one's birth. We can compute\ncohorts on the basis of names from Wikipedia, or Encyclopedia Britannica. In Figure 5, we used cohorts\ncomputed with names from Wikipedia.\nAt each time point, we defined the frequency of the cohort as the median value of the frequencies of all\nindividuals in the cohort.\nFor each cohort, we define:\n(1) Age of initial celebrity. This is the first age when the cohort's frequency is greater than 10-9. This\ncorresponds to the point at which the median individual in the cohort enter the \"English lexicon\" as\ndefined in the first section of the paper.\n(2) Age of peak celebrity. This is the first age when the cohort's frequency is greater than 95% of its peak\nvalue. This definition is meant to diminish the noise that exists on the exact position of the peak value of\nthe cohort's frequency.\n(3) Doubling time of fame. We compute the exponential rate at which fame increases between the 'age of\nfame' and the 'age of peak fame'. To do so, we fit an exponential to the timeseries with the methods of\nleast squares. The doubling time is derived from the estimated exponent.\n(4) Half-life of fame. We compute the exponential rate at which fame decreases past the year at which it\nreaches its peak (which is later than the \"age of peak celebrity\" as defined above). To do so, we fit an\nexponential to the timeseries with the methods of least squares. The half-life is derived from the\nestimated exponent.\nWe show the way these parameters change with the cohort‟s year of birth in Figure S13.\nThe dynamics of these quantities is sensibly the same when using cohorts from Wikipedia or from\nEncyclopedia Britannica. However, Britannica features fewer individuals in their cohorts, and therefore the\ncohorts from the early 19 th century are much noisier. We show in Figure S14 the fame analysis\nconducted with cohorts from Britannica, restricting our analysis to the years 1840-1950.\nIn Figure 5E, we analyze the trade-offs between early celebrity and overall fame as a function of\noccupation. For each occupation, we select the top 25 most famous individuals born between 1800 and\n1920. For each occupation, we define the contour within which all points are close to at least 2 member of\nthe cohort (it is the contour of the density map created by the cohort).\n25\nPeople leave more behind them than a name. Like her fictional protagonist Victor Frankenstein, Mary\nShelley is survived by her creation: Frankenstein took on a life of his own within our collective imagination\n(Figure S15). Such legacies, and all the many other ways in which people achieve cultural immortality,\nfall beyond the scope of this initial examination.\nIII.8. History of Technology\nA list of inventions from 1800-1960 was taken from Wikipedia (Ref S10).\nThe year listed is used in our analysis. Where multiple listings of a particular invention appear, the year\nretained in the list is the one reported in the main Wikipedia article for the invention. (e.g. \"Microwave\nOven\" is listed in 1945 and 1946; the main article lists 1945 as the year of invention, and this is the year\nwe use in our analyses).\nEach entry's main Wikipedia page was checked for alternate terms for the invention. Where alternate\nnames were listed in the main article (e.g. thiamine or thiamin or vitamin B 1 ), all the terms were\ncompared for their presence in the database. Where there was no single dominant term (e.g.MSG or\nmonosodium glutamate) the invention was eliminated from the list. If a name other than the originally\nlisted one appears to be dominant, the dominant name was used in the analysis (e.g.\nelectroencephalograph and EEG - EEG is used).\nInventions were grouped into 40-year intervals (1800-1840, 1840-1880, 1880-1920, and 1920-1960), and\nthe median percentages of peak frequency was calculated for each bin for each year following invention:\nthese were plotted in Fig 4B, together with examples of individual inventions in inset.\nOur study of the history of technology suffers from a possible sampling bias: it is possible that some older\ninventions, which peaked shortly after their invention, are by now forgotten and not listed in the Wikipedia\narticle at all. This sampling bias would be more extreme for the earlier cohorts, and would therefore tend\nto exaggerate the lag between invention date and cultural impact in the older invention cohorts. We have\nverified that our inventions are past their peaks, in all three cohorts (Fig S16). Future analyses would\nbenefit from the use of historical invention lists to control for this effect.\nAnother possible bias is that observing inventions later after they were invented leaves more room for the\nfame of these inventions to rise. To ensure that the effect we observe is not biased in this way, we\nreproduce the analysis done in the paper using constant time intervals: a hundred years from time of\ninvention. Because we have a narrower timespan, we consider only technologies invented in the 19 th\ncentury; and we group them in only two cohorts. The effect is consistent with that observed in the main\ntext (Fig S16).\nIII.9. Censorship\nIII.9A. Comparing the influence of censorship and propaganda on\nvarious groups\nTo create panel E of Fig 6, we analyzed a series of cohorts; for each cohort, we display the mean of the\nnormalized probability mass functions of the cohort, as described in section 1B. We multiplied the result\nby 100 in order to represent the probability mass functions more intuitively, as a percentage of lifetime\n26\nfame. People whose names did not appear in the cohorts for the time periods in question (1925-1933,\n1933-1945, and 1955-1965) were eliminated from the analysis.\nThe cohorts we generated were based on four major sources, and their content is given in Appendix.\n1) The Hermann lists\nThe lists of the infamous librarian Wolfgang Hermann were originally published in a librarianship journal\nand later in Boersenblatt, a publishing industry magazine in Germany. They are reproduced in Ref S11. A\ndigital version is available on the German-language version of Wikipedia (Ref S12). We considered\ndigitizing Ref S10 by hand to ensure accuracy, but felt that both OCR and manual entry would be timeconsuming\nand error prone. Consequently, we began with the list available on Wikipedia and hired a\nmanual annotator to compare this list with the version appearing in Ref S11 to ensure the accuracy of the\nresulting list. The annotator did not have access to our data and made these decisions purely on the\nbasis of the text of Ref S11. The following changes were made:\nLiterature\n1) “Fjodor Panfjorow” was changed to “Fjodor Panferov”.\n2) “Nelly Sachs” was deleted.\nHistory\n1) “Hegemann W. Ellwald, Fr. v.” was changed to “W. Hegemann” and “Fr. Von Hellwald”\nArt\n4) “Paul Stefan” was deleted.\nPhilosophy/Religion\n1) “Max Nitsche” was deleted.\nThe results of this manual correction process were used as our lists for Politics, Literature, Literary\nHistory, History, Art-related Writers, and Philosophy/Religion.\n27\n2) The Berlin list\nThe lists of Hermann continued to be expanded by the Nazi regime. We also analyzed a version from\n1938 (Ref S13). This version was digitized by the City of Berlin to mark the 75 th year after the book\nburnings in 2008 (Ref S14). The list of authors appearing on the website occasionally included multiple\nauthors on a single line, or errors in which the author field did not actually contain the name of a person\nwho wrote the text. These were corrected by hand to create an initial list.\nWe noted that many authors were listed only using a last name and a first initial. Our manual annotator\nattempted to determine the full name of any such author. The results were far from comprehensive, but\ndid lead us to expand the dataset somewhat; names with only first initials were replaced by the full name\nwherever possible.\nSome authors were listed using a pseudonym, and on several occasions our manual annotator was able\nto determine the real name of the author who used a given pseudonym. In this case, the real name was\nadded to the list.\nIn addition, we occasionally included multiple spelling variants for a single author. Because of this, and\nbecause an author‟s real name and pseudonym may both be included on the list, the number of author\nnames on the list very slightly exceeds the number of individuals being examined. The numbers reported\nin the figure are the number of names on the list.\nIt is worth pointing out that Adolf Hitler appears as an author of one of the banned books from 1938. This\nis due to a French version of Mein Kampf, together with commentary, which was banned by the Nazi\nauthorities. Although it is extremely peculiar to find Hitler on a list of banned authors, we did not remove\nHitler‟s name, as we had no basis for doing so from the standpoint of the technical authorship and name\ncriteria described above: Adolf Hitler is indeed listed as the author of a book that was banned by the Nazi\nregime. This is consistent with our stance throughout the paper, which is that we avoided making\njudgments ourselves that could bias the outcome of our results. Instead, we relied strictly upon our\nsecondary sources. Because Adolf Hitler is only one of many names, the list as a whole nevertheless\nexhibits strong evidence of suppression, especially because the measure we retained (median usage) is\nrobust to such outliers.\n3) Degenerate artists\nThe list of degenerate artists was taken directly from the catalog of a recent exhibition at the Los Angeles\nCounty Museum of Art which endeavored to reconstruct the original „Degenerate Art‟ exhibition (Ref S15).\n4) People with recorded ties to Nazis\nThe list of Nazi party members was generated in a manner consistent with the occupation categories in\nsection 7. We included the following Wikipedia categories: Nazis_from_outside_Germany, Nazi_leaders,\nSS_officers, Holocaust_perpetrators, Officials_of_Nazi_Germany, Nazis_convicted_of_war_crimes,\ntogether with all of their subcategories, with the exception of Nazis_from_outside_Germany. In addition,\nthe three categories German_Nazi_politicians, Nazi_physicians, Nazis were included without their\nrespective subcategories.\nIII.9B. De Novo Identification of Censored and Suppressed Individuals\nWe began with the list of 56,500 people, comprising the 500 most famous individuals born in each year\nfrom 1800 – 1913. This list was derived from the analysis of all biographies in Wikipedia described in\nsection 7. We removed all individuals whose mean frequency in the German language corpus was less\nthan 5 x 10 -9 during the period from 1925 – 1933; because their frequency is low, a statistical assessment\nof the effect of censorship and suppression on these individuals is more susceptible to noise.\nThe suppression index is computed for the remaining individuals using an observed/expected measure.\nThe expected fame for a given year is computed by taking the mean frequency of the individual in the\nGerman language from 1925-1933, and the mean frequency of the individual from 1955-1965. These two\nvalues are assigned to 1929 and 1960, respectively; linear interpolation is then performed in order to\ncompute an expected fame value in 1939. This expected value is compared to the observed mean\nfrequency in the German language during the period from 1933-1945. The ratio of these two numbers is\nthe suppression index s. The complete list of names and suppression indices is included as supplemental\ndata. The distribution of s was plotted for using a logarithmic binning strategy, with 100 bins between 10 -2\nand 10 2 . Three specific individuals who received scores indicating suppression in German are indicated\non the plot by arrows (Walter Gropius, Pablo Picasso, and Hermann Maas).\nAs a point of comparison, the entire analysis was repeated for English; these results are shown on the\nplot.\nIII.9C. Validation by an expert annotator\nWe wanted to see whether the findings of this high-throughput, quantitative approach were consistent\nwith the conclusions of an expert annotator using traditional, qualitative methods. We created a list of 100\nindividuals at the extremes of our distribution, including the names of the fifty people with the largest s\nvalue and of the fifty people with the smallest s value. We hired a guide at Yad Vashem with advanced\ndegrees in German and Jewish literature to manually annotate these 100 names based on her\nassessment of which people were suppressed by the Nazis (S), which people would have benefited from\nthe Nazi regime (B), and lastly, which people would not obviously be affected in either direction (N). All\n100 names were presented to the annotator in a single, alphabetized list; the annotator did not have\naccess to any of our methods, data, or conclusions. Thus the annotator‟s assessment is wholly\nindependent of our own.\n28\nThe annotator assigned 36 names to the S category and 27 names to the B category; the remaining 37\nwere given the ambiguous N classification. Of the names assigned to the S category by the human\nannotator, 29 had been annotated as suppressed by our algorithm, and 7 as elevated, so the\ncorrespondence between the annotator and our algorithm was 81%. Of the names assigned to the B\ncategory, 25 were annotated as elevated by our algorithm, and only 2 as suppressed, so the\ncorrespondence was 93%.\nTaken together, the conclusions of a scholarly annotator researching one name at a time closely matched\nthose of our automated approach. These findings confirm that our computational method provides an\neffective strategy for rapidly identifying likely victims of censorship given a large pool of possibilities.\nIII.10. Epidemics\nDisease epidemics have a significant impact on the surrounding culture (Fig. S18 A-C). It was recently\nshown that during seasonal influenza epidemics, users of Google are more likely to engage in influenzarelated\nsearches, and that this signature of influenza epidemics corresponds well with the results of CDC\nsurveillance (Ref S16). We therefore reasoned that culturomic approaches might be used to track\nhistorical epidemics. These could help complement historical medical records, which are often woefully\nincomplete.\nWe examined timelines for 4 diseases: influenza (main text), cholera, HIV, and poliomyelitis. In the case\nof influenza, peaks in cultural interest showed excellent correspondence with known historical epidemics\n(the Russian Flu of 1890, leading to 1M deaths, the Spanish Flu of 1918, leading to 20-100M deaths; and\nthe Asian Flu of 1957, leading to 1.5M deaths). Similar results were observed for cholera and HIV.\nHowever, results for polio were mixed. The US epidemic of 1916 is clearly observed, but the 1951-55\nepidemic is harder to pinpoint: the observed peak is much broader, starting in the 30s and ending in the\n60s. This is likely due to increased interest in polio following the election of Franklin Delano Roosevelt in\n1932, as well as the development and deployment of Salk‟s polio vaccine in 1952 and Sabin‟s oral\nversion in 1962. These confounding factors highlight the challenge of interpreting timelines of cultural\ninterest: interest may increase in response to an epidemic, but it may also respond to a stricken celebrity\nor a famous cure.\nThe dates of important historical epidemics were derived from the Cambridge World History of Human\nDiseases (1993) 3 rd Edition.\nFor cholera, we retained the time periods which most affected the Western world, according to this\nresource:\n- 1830-35 (Second Cholera Epidemic)\n- 1848-52, and 1854 (Third Cholera Epidemic)\n- 1866-74 (Fourth Cholera Epidemic)\n- 1883-1887 (Fifth Cholera Epidemic)\nThe first, sixth and seventh cholera epidemics appear not to have caused significant casualties in the\nWestern world.\n29\nSupplementary References\n“Quantitative analysis of culture using millions of digitized books”,\nMichel et al.\nS1. L. Taycher, “Books of the world stand up and be counted”,\n2010. http://booksearch.blogspot.com/2010/08/books-of-world-stand-up-and-becounted.html\nS2. Ray Smith, Daria Antonova, and Dar-Shyang Lee, Adapting the Tesseract\nopen source OCR engine for multilingual OCR, Proceedings of the\nInternational Conference on Multilingual OCR, Barcelona Spain, 2009,\nhttp://doi.acm.org/10.1145/1577802.1577804\nS3. Popat, Ashok. \"A panlingual anomalous text detector.\" DocEng '09: Proceedings\nof the 9th ACM symposium on Document Engineering, 2009, pp. 201-204.\nS4. Brants, Thorsten and Franz, Alex. \"Web 1T 5-gram Version 1.\" LDC2006T13\nhttp://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006T13\nS5. Dean, Jeffrey and Ghemawat, Sanjay. \"MapReduce: Simplified Data Processing\non Large Clusters.\" OSDI '04 p137--150\nS6. Lyman, Peter and Hal R. Varian, \"How Much Information\", 2003.\nhttp://www2.sims.berkeley.edu/research/projects/how-much-info-\n2003/print.htm#books\nS7. http://en.wikipedia.org/wiki/List_of_treaties.\nS8. http://en.wikipedia.org/wiki/Geographical_renaming]\nS9. Christian Bizer, Jens Lehmann, Georgi Kobilarov, Sören Auer, Christian Becker,\nRichard Cyganiak, Sebastian Hellmann.” DBpedia – A Crystallization Point for\nthe Web of Data.” Journal of Web Semantics: Science, Services and Agents on\nthe World Wide Web, 2009, pp. 154–165.\nS10. http://en.wikipedia.org/wiki/Timeline_of_historic_inventions\nS11. Gerhard Sauder: Die Bücherverbrennung.10. Mai 1933. Ullstein Verlag, Berlin,\nWien 1985.\nS12. http://de.wikipedia.org/wiki/Liste_der_verbrannten_Bücher_1933.\nS13. Liste Des Schädlichen Und Unerwünschten Schrifttums: Stand Vom 31. Dez.\n1938. Leipzig: Hedrich, 1938. Print.\nS14. http://www.berlin.de/rubrik/hauptstadt/verbannte_buecher/az-autor.php\nS15. Barron, Stephanie, and Peter W. Guenther. Degenerate Art: the Fate of the\nAvant-garde in Nazi Germany. Los Angeles, CA: Los Angeles County Museum\nof Art, 1991. Print.\nS16. Ginsberg, Jeremy, Matthew H. Mohebbi, Rajan S. Patel, Lynnette Brammer,\nMark S. Smolinski, and Larry Brilliant. \"Detecting Influenza Epidemics Using\nSearch Engine Query Data.\" Nature 457 (2008): 1012-014.\nSupplementary Figures\n“Quantitative analysis of culture using millions of digitized books”,\nMichel et al.\nFigure S1\nFig. S1. Schematic of stereo scanning for Google Books.\nFigure S2\nFig. S2. Example of a page scanned before (left) and after processing (right).\nFigure S3\nFig. S3. Outline of n-gram corpus construction. The numbering corresponds to sections of the text.\nFigure S4\nFig. S4. Fraction of English Books with a given OCR quality.\nFigure S5\nFig. S5. Known events exhibit sharp peaks at date of occurrence. We select groups of events that occur\nat known dates, and produce the corresponding timeseries. We normalize each timeserie relative to its\ntotal frequency, center the timeseries around the relevant event, and plot the mean. (A) A list of 124\ntreaties. (B) A list of 43 head of state (US presidents, UK monarchs), centered around the year when they\nwere elected president or became king/queen. (C) A list of 28 country name changes, centered around\nthe year of name change. Together, these form positive controls about timeseries in the corpus.\nFigure S6\nFig. S6. Frequency distribution of words in the dictionary. We compute the frequency in our year 2000\nlexicon for all 116,156 words (1-grams) in the AHD (year 2000). We represent the percentage of these\nwords whose frequency is smaller than the value on the x-axis (logarithmic scale, base 10). 90% of all\nwords in AHD are more frequent than 1 part per billion (10 -9 ), but only 75% are more frequent than 1\npart per 100 million (10 -8 ).\nFigure S7\nFig. S7. Lexical trends excluding proper nouns. We compute the number of words that are 1-grams in\nthe categories “P”, “B” and “R”. The same upward trend starting in 1950 is observed. The size of the\nlexicon in the year 2000 is still larger than the OED or W3.\nFigure S8\nFig. S8. Example of grammatical change. Irregular verbs are used as a model of grammatical evolution.\nFor each verb, we plot the usage frequency of its irregular form in red (for instance, ‘found’), and the\nusage frequency of its regular past-tense form in blue (for instance, ‘finded’). Virtually all irregular verbs\nare found from time to time used in a regular form, but those used more often tend to be used in a\nregular way more rarely. This is illustrated in the top two rows with the frequently-used verb “find” and\nthe less often encountered “dwell”. In the third row, the trajectory of “thrive” is one of many ways by\nwhich regularization occurs. The bottom two panels shows that the regularization of “spill” happened\nearlier in the US than in the UK.\nFigure S9\nFig. S9. We forget. Events of importance provoke a peak of discussion shortly after they happened, but\ninterest in them quickly decreases.\nFigure S10\nFig. S10. Biographical Records. The number of records parsed from the two encyclopedic sources (blue\ncurve), and used in our analyses (green curve). See steps 7.A.1 to 7.A.10 above.\nFigure S11\nFig. S11. Selection of query name. The chosen query name is in black. (A) Adrien Albert Marie de Mun.\nStrongest and optimal query name is Albert de Mun, (B) Oliver Joseph Lodge, strongest and optimal\nquery name is Oliver Lodge, (C) Henry David Thoreau. Strongest query name is David Thoreau, but is a\nsubstring match of Henry David Thoreau, with fame >80% of David Thoreau. Optimal query name is\nHenry David Thoreau. (D) Mary Tyler Moore. Strongest name is Mary Moore, but is rejected because of\nnoise. Next strongest is Tyler Moor, but this is a substring match of Mary Tyler Moore, with fame >80%\nof Tyler Moore. Optimal query name is thus Mary Tyler Moore.\nFigure S12\nFig. S12. Filtering out names with trajectories that cannot be resolved. Illustrates the requirement for\nquery name filtration on the basis of premature fame. Fame a birth is the average fame in a 10 year\nwindow around birth, lifetime fame is the average fame from year of birth to 2000. The dashed line in\n(A), (D) indicates the separatrix used to excluded query names with premature fame signals. Points to\nthe right were rejected from further analysis. In (B), (C), (E), (F) the black line indicates the year of birth\nof the individuals whose fame trajectories are plotted.\nFigure S13\nFig. S13. Values of the four parameters of fame as a function of time. ‘Age of peak celebrity’ (75 years\nold) has been fairly consistent. Celebrities are noticed earlier, and become more famous than ever\nbefore: ‘Age of initial celebrity’ has dropped from 43 to 29 years, and ‘Doubling time’ has dropped from\n8.1 to 3.3 years. But they are forgotten sooner as well: the half-life has declined from 120 years to 71.\nFigure S14\nFig. S14. Fundamental parameters of fame do not depend on the underlying source of people studied.\nWe represent the analysis of fame using individuals from Encyclopedia Britannica.\nFigure S15\nFig. S15. Many routes to Immortality. People leave more behind them than their name: ‘Mary Shelley’\n(blue) created the monstrously famous ‘Frankenstein’ (green).\nFigure S16\nFig. S16. Controls. (A) We observe over the same timespan (100 years) two cohorts invented at different\ntimes. Again, the more recent cohort reaches 25% of its peak faster. (B) We verify that inventions have\nalready reached their peak. We calculate the peak of each invention, and plot the distribution of these\npeaks as a function of year, grouping them along the same cohorts as used in the text. In each case, the\ndistribution falls within the bounds of the period observed (1800-2000).\nFigure S17\nFig. S17. Suppression of authors on the Art and Literary History blacklists in German. We plot the\nmedian trajectory (as in the main text) of authors in the Herman lists for Art (green) and Literary History\n(red), and for authors found in the 1938 blacklist (blue). The Nazi regime (1933-1945) is highlighted, and\ncorresponds to strong drops in the trajectories of these authors.\nFigure S18\nFig. S18. Tracking historical epidemics using their influence on the surrounding culture. (A) Usage\nfrequency of various diseases: ‘fever’ (blue), ‘cancer’ (green), ‘asthma’ (red), ‘tuberculosis’ (cyan),\n‘diabetes’ (purple), ‘obesity’ (yellow) and ‘heart attack’ (black). (B) Cultural prevalence of AIDS and HIV.\nWe highlight the year 1983 when the viral agent was discovered. (C) Usage of the term ‘cholera’ peaks\nduring the cholera epidemics that affected Europe and the US (blue shading). (D) Usage of the term\n‘infantile paralysis’ (blue) exhibits one peak during the 1916 polio epidemic (blue shading), and a second\naround the time of a series of polio epidemics that took place during the early 1950s. But the second\npeak is anomalously broad. Discussion of polio during that time may have been fueled by the election of\n‘Franklin Delano Roosevelt’ (green), who had been paralyzed by polio in 1936 (green shading), as well as\nby the development of the ‘polio vaccine’ (red) in 1952. The vaccine ultimately eradicated ‘infantile\nparalysis’ in the United States.\nFigure S19\nFig. S19. Culturomic ‘timelines’ reveal how often a word or phrase appears in books over time. (A) ‘civil\nrights’, ‘women’s rights’, ‘children’s rights’ and ‘animals rights’ are shown. (B) ‘genocide’ (blue), ‘the\nHolocaust’ (green), and ‘ethnic cleansing’ (red) (C) Ideology: ideas about ‘capitalism’ (blue) and\n‘communism’ (green) became extremely important during the 20 th century. The latter peaked during the\n1950s and 1960s, but is now decreasing. Sadly, ‘terrorism’ (red) has been on the rise. (D) Climate\nchange: Awareness of ‘global temperature’, ‘atmospheric CO2’, and ‘sea levels’ is increasing. (E) ‘aspirin’\n(blue), ‘penicillin’ (green), ‘antibiotics’ (red), and ‘quinine’ (cyan). (F) ‘germs’ (blue), ‘hygiene’ (green)\nand ‘sterilization’ (red). (G) The history of economics: ‘banking’ (blue) is an old concept which was of\ncentral concern during ‘the depression’ (red). Afterwards, a new economic vocabulary arose to\nsupplement the older ideas. New concepts such as ‘recession’ (cyan), ‘GDP’ (purple), and ‘the economy’\n(green) entered everyday discourse. (H) We illustrate geographical name changes: ‘Upper Volta’ (blue)\nand ‘Burkina Faso’ (green). (I) ‘radio’ in the US (blue) and in the UK (red) have distinct trajectories. (J)\n‘football’ (blue), ‘golf’ (green), ‘baseball’ (red), ‘basketball’ (cyan) and ‘hockey’ (purple) (K) Sportsmen: In\nthe 1980s, the fame of ‘Michael Jordan’ (cyan) leaped over other that of other great athletes, including\n‘Jesse Owens’ (green), ‘Joe Namath’ (red), ‘Mike Tyson’ (purple), and ‘Wayne Gretsky’ (yellow).\nPresently, only ‘Babe Ruth’ (blue) can compete. One can only speculate as to whether Jordan’s hang\ntime will match that of the Bambino. (L) ‘humorless’ is a word that rose to popularity during the first half\nof the century. This indicates how these data can serve to identify words that are a marker of a specific\nperiod in time.\n"
  },
  "error": "LLM request failed: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\\nPlease retry in 29.325043289s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-3-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '29s'}]}}",
  "house_oversight_id": "016996",
  "processing_metadata": {
    "processed_at": "2025-12-20T00:22:30.795908Z",
    "model": "gemini-3-flash-preview"
  }
}